{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16f8600e-8add-4274-bc6f-96877071f4f1",
   "metadata": {},
   "source": [
    "# retriving the data from chembl dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82c17a18-bd7c-4c8a-a593-33bb0d464a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for Acetylcholinesterase (AChE) target...\n",
      "Found target: CHEMBL220\n",
      "Fetching ligand activities...\n",
      "Retrieved 2000 activities so far... (offset: 2000)\n",
      "Retrieved 2020 activities so far... (offset: 2020)\n",
      "Retrieved 2040 activities so far... (offset: 2040)\n",
      "Retrieved 2060 activities so far... (offset: 2060)\n",
      "Retrieved 2080 activities so far... (offset: 2080)\n",
      "Retrieved 2100 activities so far... (offset: 2100)\n",
      "Retrieved 2120 activities so far... (offset: 2120)\n",
      "Retrieved 2140 activities so far... (offset: 2140)\n",
      "Retrieved 2160 activities so far... (offset: 2160)\n",
      "Retrieved 2180 activities so far... (offset: 2180)\n",
      "Retrieved 2200 activities so far... (offset: 2200)\n",
      "Retrieved 2220 activities so far... (offset: 2220)\n",
      "Retrieved 2240 activities so far... (offset: 2240)\n",
      "Retrieved 2260 activities so far... (offset: 2260)\n",
      "Retrieved 2280 activities so far... (offset: 2280)\n",
      "Retrieved 2300 activities so far... (offset: 2300)\n",
      "Retrieved 2320 activities so far... (offset: 2320)\n",
      "Retrieved 2340 activities so far... (offset: 2340)\n",
      "Retrieved 2360 activities so far... (offset: 2360)\n",
      "Retrieved 2380 activities so far... (offset: 2380)\n",
      "Retrieved 2400 activities so far... (offset: 2400)\n",
      "Retrieved 2420 activities so far... (offset: 2420)\n",
      "Retrieved 2440 activities so far... (offset: 2440)\n",
      "Retrieved 2460 activities so far... (offset: 2460)\n",
      "Retrieved 2480 activities so far... (offset: 2480)\n",
      "Retrieved 2500 activities so far... (offset: 2500)\n",
      "Retrieved 2520 activities so far... (offset: 2520)\n",
      "Retrieved 2540 activities so far... (offset: 2540)\n",
      "Retrieved 2560 activities so far... (offset: 2560)\n",
      "Retrieved 2580 activities so far... (offset: 2580)\n",
      "Retrieved 2600 activities so far... (offset: 2600)\n",
      "Retrieved 2620 activities so far... (offset: 2620)\n",
      "Retrieved 2640 activities so far... (offset: 2640)\n",
      "Retrieved 2660 activities so far... (offset: 2660)\n",
      "Retrieved 2680 activities so far... (offset: 2680)\n",
      "Retrieved 2700 activities so far... (offset: 2700)\n",
      "Retrieved 2720 activities so far... (offset: 2720)\n",
      "Retrieved 2740 activities so far... (offset: 2740)\n",
      "Retrieved 2760 activities so far... (offset: 2760)\n",
      "Retrieved 2780 activities so far... (offset: 2780)\n",
      "Retrieved 2800 activities so far... (offset: 2800)\n",
      "Retrieved 2820 activities so far... (offset: 2820)\n",
      "Retrieved 2840 activities so far... (offset: 2840)\n",
      "Retrieved 2860 activities so far... (offset: 2860)\n",
      "Retrieved 2880 activities so far... (offset: 2880)\n",
      "Retrieved 2900 activities so far... (offset: 2900)\n",
      "Retrieved 2920 activities so far... (offset: 2920)\n",
      "Retrieved 2940 activities so far... (offset: 2940)\n",
      "Retrieved 2960 activities so far... (offset: 2960)\n",
      "Retrieved 2980 activities so far... (offset: 2980)\n",
      "Retrieved 3000 activities so far... (offset: 3000)\n",
      "Retrieved 3020 activities so far... (offset: 3020)\n",
      "Retrieved 3040 activities so far... (offset: 3040)\n",
      "Retrieved 3060 activities so far... (offset: 3060)\n",
      "Retrieved 3080 activities so far... (offset: 3080)\n",
      "Retrieved 3100 activities so far... (offset: 3100)\n",
      "Retrieved 3120 activities so far... (offset: 3120)\n",
      "Retrieved 3140 activities so far... (offset: 3140)\n",
      "Retrieved 3160 activities so far... (offset: 3160)\n",
      "Retrieved 3180 activities so far... (offset: 3180)\n",
      "Retrieved 3200 activities so far... (offset: 3200)\n",
      "Retrieved 3220 activities so far... (offset: 3220)\n",
      "Retrieved 3240 activities so far... (offset: 3240)\n",
      "Retrieved 3260 activities so far... (offset: 3260)\n",
      "Retrieved 3280 activities so far... (offset: 3280)\n",
      "Retrieved 3300 activities so far... (offset: 3300)\n",
      "Retrieved 3320 activities so far... (offset: 3320)\n",
      "Retrieved 3340 activities so far... (offset: 3340)\n",
      "Retrieved 3360 activities so far... (offset: 3360)\n",
      "Retrieved 3380 activities so far... (offset: 3380)\n",
      "Retrieved 3400 activities so far... (offset: 3400)\n",
      "Retrieved 3420 activities so far... (offset: 3420)\n",
      "Retrieved 3440 activities so far... (offset: 3440)\n",
      "Retrieved 3460 activities so far... (offset: 3460)\n",
      "Retrieved 3480 activities so far... (offset: 3480)\n",
      "Retrieved 3500 activities so far... (offset: 3500)\n",
      "Retrieved 3520 activities so far... (offset: 3520)\n",
      "Retrieved 3540 activities so far... (offset: 3540)\n",
      "Retrieved 3560 activities so far... (offset: 3560)\n",
      "Retrieved 3580 activities so far... (offset: 3580)\n",
      "Retrieved 3600 activities so far... (offset: 3600)\n",
      "Retrieved 3620 activities so far... (offset: 3620)\n",
      "Retrieved 3640 activities so far... (offset: 3640)\n",
      "Retrieved 3660 activities so far... (offset: 3660)\n",
      "Retrieved 3680 activities so far... (offset: 3680)\n",
      "Retrieved 3700 activities so far... (offset: 3700)\n",
      "Retrieved 3720 activities so far... (offset: 3720)\n",
      "Retrieved 3740 activities so far... (offset: 3740)\n",
      "Retrieved 3760 activities so far... (offset: 3760)\n",
      "Retrieved 3780 activities so far... (offset: 3780)\n",
      "Retrieved 3800 activities so far... (offset: 3800)\n",
      "Retrieved 3820 activities so far... (offset: 3820)\n",
      "Retrieved 3840 activities so far... (offset: 3840)\n",
      "Retrieved 3860 activities so far... (offset: 3860)\n",
      "Retrieved 3880 activities so far... (offset: 3880)\n",
      "Retrieved 3900 activities so far... (offset: 3900)\n",
      "Retrieved 3920 activities so far... (offset: 3920)\n",
      "Retrieved 3940 activities so far... (offset: 3940)\n",
      "Retrieved 3960 activities so far... (offset: 3960)\n",
      "Retrieved 3980 activities so far... (offset: 3980)\n",
      "Retrieved 4000 activities so far... (offset: 4000)\n",
      "Retrieved 4020 activities so far... (offset: 4020)\n",
      "Retrieved 4040 activities so far... (offset: 4040)\n",
      "Retrieved 4060 activities so far... (offset: 4060)\n",
      "Retrieved 4080 activities so far... (offset: 4080)\n",
      "Retrieved 4100 activities so far... (offset: 4100)\n",
      "Retrieved 4120 activities so far... (offset: 4120)\n",
      "Retrieved 4140 activities so far... (offset: 4140)\n",
      "Retrieved 4160 activities so far... (offset: 4160)\n",
      "Retrieved 4180 activities so far... (offset: 4180)\n",
      "Retrieved 4200 activities so far... (offset: 4200)\n",
      "Retrieved 4220 activities so far... (offset: 4220)\n",
      "Retrieved 4240 activities so far... (offset: 4240)\n",
      "Retrieved 4260 activities so far... (offset: 4260)\n",
      "Retrieved 4280 activities so far... (offset: 4280)\n",
      "Retrieved 4300 activities so far... (offset: 4300)\n",
      "Retrieved 4320 activities so far... (offset: 4320)\n",
      "Retrieved 4340 activities so far... (offset: 4340)\n",
      "Retrieved 4360 activities so far... (offset: 4360)\n",
      "Retrieved 4380 activities so far... (offset: 4380)\n",
      "Retrieved 4400 activities so far... (offset: 4400)\n",
      "Retrieved 4420 activities so far... (offset: 4420)\n",
      "Retrieved 4440 activities so far... (offset: 4440)\n",
      "Retrieved 4460 activities so far... (offset: 4460)\n",
      "Retrieved 4480 activities so far... (offset: 4480)\n",
      "Retrieved 4500 activities so far... (offset: 4500)\n",
      "Retrieved 4520 activities so far... (offset: 4520)\n",
      "Retrieved 4540 activities so far... (offset: 4540)\n",
      "Retrieved 4560 activities so far... (offset: 4560)\n",
      "Retrieved 4580 activities so far... (offset: 4580)\n",
      "Retrieved 4600 activities so far... (offset: 4600)\n",
      "Retrieved 4620 activities so far... (offset: 4620)\n",
      "Retrieved 4640 activities so far... (offset: 4640)\n",
      "Retrieved 4660 activities so far... (offset: 4660)\n",
      "Retrieved 4680 activities so far... (offset: 4680)\n",
      "Retrieved 4700 activities so far... (offset: 4700)\n",
      "Retrieved 4720 activities so far... (offset: 4720)\n",
      "Retrieved 4740 activities so far... (offset: 4740)\n",
      "Retrieved 4760 activities so far... (offset: 4760)\n",
      "Retrieved 4780 activities so far... (offset: 4780)\n",
      "Retrieved 4800 activities so far... (offset: 4800)\n",
      "Retrieved 4820 activities so far... (offset: 4820)\n",
      "Retrieved 4840 activities so far... (offset: 4840)\n",
      "Retrieved 4860 activities so far... (offset: 4860)\n",
      "Retrieved 4880 activities so far... (offset: 4880)\n",
      "Retrieved 4900 activities so far... (offset: 4900)\n",
      "Retrieved 4920 activities so far... (offset: 4920)\n",
      "Retrieved 4940 activities so far... (offset: 4940)\n",
      "Retrieved 4960 activities so far... (offset: 4960)\n",
      "Retrieved 4980 activities so far... (offset: 4980)\n",
      "Retrieved 5000 activities so far... (offset: 5000)\n",
      "Retrieved 5020 activities so far... (offset: 5020)\n",
      "Retrieved 5040 activities so far... (offset: 5040)\n",
      "Retrieved 5060 activities so far... (offset: 5060)\n",
      "Retrieved 5080 activities so far... (offset: 5080)\n",
      "Retrieved 5100 activities so far... (offset: 5100)\n",
      "Retrieved 5120 activities so far... (offset: 5120)\n",
      "Retrieved 5140 activities so far... (offset: 5140)\n",
      "Retrieved 5160 activities so far... (offset: 5160)\n",
      "Retrieved 5180 activities so far... (offset: 5180)\n",
      "Retrieved 5200 activities so far... (offset: 5200)\n",
      "Retrieved 5220 activities so far... (offset: 5220)\n",
      "Retrieved 5240 activities so far... (offset: 5240)\n",
      "Retrieved 5260 activities so far... (offset: 5260)\n",
      "Retrieved 5280 activities so far... (offset: 5280)\n",
      "Retrieved 5300 activities so far... (offset: 5300)\n",
      "Retrieved 5320 activities so far... (offset: 5320)\n",
      "Retrieved 5340 activities so far... (offset: 5340)\n",
      "Retrieved 5360 activities so far... (offset: 5360)\n",
      "Retrieved 5380 activities so far... (offset: 5380)\n",
      "Retrieved 5400 activities so far... (offset: 5400)\n",
      "Retrieved 5420 activities so far... (offset: 5420)\n",
      "Retrieved 5440 activities so far... (offset: 5440)\n",
      "Retrieved 5460 activities so far... (offset: 5460)\n",
      "Retrieved 5480 activities so far... (offset: 5480)\n",
      "Retrieved 5500 activities so far... (offset: 5500)\n",
      "Retrieved 5520 activities so far... (offset: 5520)\n",
      "Retrieved 5540 activities so far... (offset: 5540)\n",
      "Retrieved 5560 activities so far... (offset: 5560)\n",
      "Retrieved 5580 activities so far... (offset: 5580)\n",
      "Retrieved 5600 activities so far... (offset: 5600)\n",
      "Retrieved 5620 activities so far... (offset: 5620)\n",
      "Retrieved 5640 activities so far... (offset: 5640)\n",
      "Retrieved 5660 activities so far... (offset: 5660)\n",
      "Retrieved 5680 activities so far... (offset: 5680)\n",
      "Retrieved 5700 activities so far... (offset: 5700)\n",
      "Retrieved 5720 activities so far... (offset: 5720)\n",
      "Retrieved 5740 activities so far... (offset: 5740)\n",
      "Retrieved 5760 activities so far... (offset: 5760)\n",
      "Retrieved 5780 activities so far... (offset: 5780)\n",
      "Retrieved 5800 activities so far... (offset: 5800)\n",
      "Retrieved 5820 activities so far... (offset: 5820)\n",
      "Retrieved 5840 activities so far... (offset: 5840)\n",
      "Retrieved 5860 activities so far... (offset: 5860)\n",
      "Retrieved 5880 activities so far... (offset: 5880)\n",
      "Retrieved 5900 activities so far... (offset: 5900)\n",
      "Retrieved 5920 activities so far... (offset: 5920)\n",
      "Retrieved 5940 activities so far... (offset: 5940)\n",
      "Retrieved 5960 activities so far... (offset: 5960)\n",
      "Retrieved 5980 activities so far... (offset: 5980)\n",
      "Retrieved 6000 activities so far... (offset: 6000)\n",
      "Retrieved 6020 activities so far... (offset: 6020)\n",
      "Retrieved 6040 activities so far... (offset: 6040)\n",
      "Retrieved 6060 activities so far... (offset: 6060)\n",
      "Retrieved 6080 activities so far... (offset: 6080)\n",
      "Retrieved 6100 activities so far... (offset: 6100)\n",
      "Retrieved 6120 activities so far... (offset: 6120)\n",
      "Retrieved 6140 activities so far... (offset: 6140)\n",
      "Retrieved 6160 activities so far... (offset: 6160)\n",
      "Retrieved 6180 activities so far... (offset: 6180)\n",
      "Retrieved 6200 activities so far... (offset: 6200)\n",
      "Retrieved 6220 activities so far... (offset: 6220)\n",
      "Retrieved 6240 activities so far... (offset: 6240)\n",
      "Retrieved 6260 activities so far... (offset: 6260)\n",
      "Retrieved 6280 activities so far... (offset: 6280)\n",
      "Retrieved 6300 activities so far... (offset: 6300)\n",
      "Retrieved 6320 activities so far... (offset: 6320)\n",
      "Retrieved 6340 activities so far... (offset: 6340)\n",
      "Retrieved 6360 activities so far... (offset: 6360)\n",
      "Retrieved 6380 activities so far... (offset: 6380)\n",
      "Retrieved 6400 activities so far... (offset: 6400)\n",
      "Retrieved 6420 activities so far... (offset: 6420)\n",
      "Retrieved 6440 activities so far... (offset: 6440)\n",
      "Retrieved 6460 activities so far... (offset: 6460)\n",
      "Retrieved 6480 activities so far... (offset: 6480)\n",
      "Retrieved 6500 activities so far... (offset: 6500)\n",
      "Retrieved 6520 activities so far... (offset: 6520)\n",
      "Retrieved 6540 activities so far... (offset: 6540)\n",
      "Retrieved 6560 activities so far... (offset: 6560)\n",
      "Retrieved 6580 activities so far... (offset: 6580)\n",
      "Retrieved 6600 activities so far... (offset: 6600)\n",
      "Retrieved 6620 activities so far... (offset: 6620)\n",
      "Retrieved 6640 activities so far... (offset: 6640)\n",
      "Retrieved 6660 activities so far... (offset: 6660)\n",
      "Retrieved 6680 activities so far... (offset: 6680)\n",
      "Retrieved 6700 activities so far... (offset: 6700)\n",
      "Retrieved 6720 activities so far... (offset: 6720)\n",
      "Retrieved 6740 activities so far... (offset: 6740)\n",
      "Retrieved 6760 activities so far... (offset: 6760)\n",
      "Retrieved 6780 activities so far... (offset: 6780)\n",
      "Retrieved 6800 activities so far... (offset: 6800)\n",
      "Retrieved 6820 activities so far... (offset: 6820)\n",
      "Retrieved 6840 activities so far... (offset: 6840)\n",
      "Retrieved 6860 activities so far... (offset: 6860)\n",
      "Retrieved 6880 activities so far... (offset: 6880)\n",
      "Retrieved 6900 activities so far... (offset: 6900)\n",
      "Retrieved 6920 activities so far... (offset: 6920)\n",
      "Retrieved 6940 activities so far... (offset: 6940)\n",
      "Retrieved 6960 activities so far... (offset: 6960)\n",
      "Retrieved 6980 activities so far... (offset: 6980)\n",
      "Retrieved 7000 activities so far... (offset: 7000)\n",
      "Retrieved 7020 activities so far... (offset: 7020)\n",
      "Retrieved 7040 activities so far... (offset: 7040)\n",
      "Retrieved 7060 activities so far... (offset: 7060)\n",
      "Retrieved 7080 activities so far... (offset: 7080)\n",
      "Retrieved 7100 activities so far... (offset: 7100)\n",
      "Retrieved 7120 activities so far... (offset: 7120)\n",
      "Retrieved 7140 activities so far... (offset: 7140)\n",
      "Retrieved 7160 activities so far... (offset: 7160)\n",
      "Retrieved 7180 activities so far... (offset: 7180)\n",
      "Retrieved 7200 activities so far... (offset: 7200)\n",
      "Retrieved 7220 activities so far... (offset: 7220)\n",
      "Retrieved 7240 activities so far... (offset: 7240)\n",
      "Retrieved 7260 activities so far... (offset: 7260)\n",
      "Retrieved 7280 activities so far... (offset: 7280)\n",
      "Retrieved 7300 activities so far... (offset: 7300)\n",
      "Retrieved 7320 activities so far... (offset: 7320)\n",
      "Retrieved 7340 activities so far... (offset: 7340)\n",
      "Retrieved 7360 activities so far... (offset: 7360)\n",
      "Retrieved 7380 activities so far... (offset: 7380)\n",
      "Retrieved 7400 activities so far... (offset: 7400)\n",
      "Retrieved 7420 activities so far... (offset: 7420)\n",
      "Retrieved 7440 activities so far... (offset: 7440)\n",
      "Retrieved 7460 activities so far... (offset: 7460)\n",
      "Retrieved 7480 activities so far... (offset: 7480)\n",
      "Retrieved 7500 activities so far... (offset: 7500)\n",
      "Retrieved 7520 activities so far... (offset: 7520)\n",
      "Retrieved 7540 activities so far... (offset: 7540)\n",
      "Retrieved 7560 activities so far... (offset: 7560)\n",
      "Retrieved 7580 activities so far... (offset: 7580)\n",
      "Retrieved 7600 activities so far... (offset: 7600)\n",
      "Retrieved 7620 activities so far... (offset: 7620)\n",
      "Retrieved 7640 activities so far... (offset: 7640)\n",
      "Retrieved 7660 activities so far... (offset: 7660)\n",
      "Retrieved 7680 activities so far... (offset: 7680)\n",
      "Retrieved 7700 activities so far... (offset: 7700)\n",
      "Retrieved 7720 activities so far... (offset: 7720)\n",
      "Retrieved 7740 activities so far... (offset: 7740)\n",
      "Retrieved 7760 activities so far... (offset: 7760)\n",
      "Retrieved 7780 activities so far... (offset: 7780)\n",
      "Retrieved 7800 activities so far... (offset: 7800)\n",
      "Retrieved 7820 activities so far... (offset: 7820)\n",
      "Retrieved 7840 activities so far... (offset: 7840)\n",
      "Retrieved 7860 activities so far... (offset: 7860)\n",
      "Retrieved 7880 activities so far... (offset: 7880)\n",
      "Retrieved 7900 activities so far... (offset: 7900)\n",
      "Retrieved 7920 activities so far... (offset: 7920)\n",
      "Retrieved 7940 activities so far... (offset: 7940)\n",
      "Retrieved 7960 activities so far... (offset: 7960)\n",
      "Retrieved 7980 activities so far... (offset: 7980)\n",
      "Retrieved 8000 activities so far... (offset: 8000)\n",
      "Retrieved 8020 activities so far... (offset: 8020)\n",
      "Retrieved 8040 activities so far... (offset: 8040)\n",
      "Retrieved 8060 activities so far... (offset: 8060)\n",
      "Retrieved 8080 activities so far... (offset: 8080)\n",
      "Retrieved 8100 activities so far... (offset: 8100)\n",
      "Retrieved 8120 activities so far... (offset: 8120)\n",
      "Retrieved 8140 activities so far... (offset: 8140)\n",
      "Retrieved 8160 activities so far... (offset: 8160)\n",
      "Retrieved 8180 activities so far... (offset: 8180)\n",
      "Retrieved 8200 activities so far... (offset: 8200)\n",
      "Retrieved 8220 activities so far... (offset: 8220)\n",
      "Retrieved 8240 activities so far... (offset: 8240)\n",
      "Retrieved 8260 activities so far... (offset: 8260)\n",
      "Retrieved 8280 activities so far... (offset: 8280)\n",
      "Retrieved 8300 activities so far... (offset: 8300)\n",
      "Retrieved 8320 activities so far... (offset: 8320)\n",
      "Retrieved 8340 activities so far... (offset: 8340)\n",
      "Retrieved 8360 activities so far... (offset: 8360)\n",
      "Retrieved 8380 activities so far... (offset: 8380)\n",
      "Retrieved 8400 activities so far... (offset: 8400)\n",
      "Retrieved 8420 activities so far... (offset: 8420)\n",
      "Retrieved 8440 activities so far... (offset: 8440)\n",
      "Retrieved 8460 activities so far... (offset: 8460)\n",
      "Retrieved 8480 activities so far... (offset: 8480)\n",
      "Retrieved 8500 activities so far... (offset: 8500)\n",
      "Retrieved 8520 activities so far... (offset: 8520)\n",
      "Retrieved 8540 activities so far... (offset: 8540)\n",
      "Retrieved 8560 activities so far... (offset: 8560)\n",
      "Retrieved 8580 activities so far... (offset: 8580)\n",
      "Retrieved 8600 activities so far... (offset: 8600)\n",
      "Retrieved 8620 activities so far... (offset: 8620)\n",
      "Retrieved 8640 activities so far... (offset: 8640)\n",
      "Retrieved 8660 activities so far... (offset: 8660)\n",
      "Retrieved 8680 activities so far... (offset: 8680)\n",
      "Retrieved 8700 activities so far... (offset: 8700)\n",
      "Retrieved 8720 activities so far... (offset: 8720)\n",
      "Retrieved 8740 activities so far... (offset: 8740)\n",
      "Retrieved 8760 activities so far... (offset: 8760)\n",
      "Retrieved 8780 activities so far... (offset: 8780)\n",
      "Retrieved 8800 activities so far... (offset: 8800)\n",
      "Retrieved 8820 activities so far... (offset: 8820)\n",
      "Retrieved 8840 activities so far... (offset: 8840)\n",
      "Retrieved 8860 activities so far... (offset: 8860)\n",
      "Retrieved 8880 activities so far... (offset: 8880)\n",
      "Retrieved 8900 activities so far... (offset: 8900)\n",
      "Retrieved 8920 activities so far... (offset: 8920)\n",
      "Retrieved 8940 activities so far... (offset: 8940)\n",
      "Retrieved 8960 activities so far... (offset: 8960)\n",
      "Retrieved 8980 activities so far... (offset: 8980)\n",
      "Retrieved 9000 activities so far... (offset: 9000)\n",
      "Retrieved 9020 activities so far... (offset: 9020)\n",
      "Retrieved 9040 activities so far... (offset: 9040)\n",
      "Retrieved 9060 activities so far... (offset: 9060)\n",
      "Retrieved 9080 activities so far... (offset: 9080)\n",
      "Retrieved 9100 activities so far... (offset: 9100)\n",
      "Retrieved 9120 activities so far... (offset: 9120)\n",
      "Retrieved 9140 activities so far... (offset: 9140)\n",
      "Retrieved 9160 activities so far... (offset: 9160)\n",
      "Retrieved 9180 activities so far... (offset: 9180)\n",
      "Retrieved 9200 activities so far... (offset: 9200)\n",
      "Retrieved 9220 activities so far... (offset: 9220)\n",
      "Retrieved 9240 activities so far... (offset: 9240)\n",
      "Retrieved 9260 activities so far... (offset: 9260)\n",
      "Retrieved 9280 activities so far... (offset: 9280)\n",
      "Retrieved 9300 activities so far... (offset: 9300)\n",
      "Retrieved 9320 activities so far... (offset: 9320)\n",
      "Retrieved 9340 activities so far... (offset: 9340)\n",
      "Retrieved 9360 activities so far... (offset: 9360)\n",
      "Retrieved 9380 activities so far... (offset: 9380)\n",
      "Retrieved 9400 activities so far... (offset: 9400)\n",
      "Retrieved 9420 activities so far... (offset: 9420)\n",
      "Retrieved 9440 activities so far... (offset: 9440)\n",
      "Retrieved 9460 activities so far... (offset: 9460)\n",
      "Retrieved 9480 activities so far... (offset: 9480)\n",
      "Retrieved 9500 activities so far... (offset: 9500)\n",
      "Retrieved 9520 activities so far... (offset: 9520)\n",
      "Retrieved 9540 activities so far... (offset: 9540)\n",
      "Retrieved 9560 activities so far... (offset: 9560)\n",
      "Retrieved 9580 activities so far... (offset: 9580)\n",
      "Retrieved 9600 activities so far... (offset: 9600)\n",
      "Retrieved 9620 activities so far... (offset: 9620)\n",
      "Retrieved 9640 activities so far... (offset: 9640)\n",
      "Retrieved 9660 activities so far... (offset: 9660)\n",
      "Retrieved 9680 activities so far... (offset: 9680)\n",
      "Retrieved 9700 activities so far... (offset: 9700)\n",
      "Retrieved 9720 activities so far... (offset: 9720)\n",
      "Retrieved 9740 activities so far... (offset: 9740)\n",
      "Retrieved 9760 activities so far... (offset: 9760)\n",
      "Retrieved 9780 activities so far... (offset: 9780)\n",
      "Retrieved 9800 activities so far... (offset: 9800)\n",
      "Retrieved 9820 activities so far... (offset: 9820)\n",
      "Retrieved 9840 activities so far... (offset: 9840)\n",
      "Retrieved 9860 activities so far... (offset: 9860)\n",
      "Retrieved 9880 activities so far... (offset: 9880)\n",
      "Retrieved 9900 activities so far... (offset: 9900)\n",
      "Retrieved 9920 activities so far... (offset: 9920)\n",
      "Retrieved 9940 activities so far... (offset: 9940)\n",
      "Retrieved 9960 activities so far... (offset: 9960)\n",
      "Retrieved 9980 activities so far... (offset: 9980)\n",
      "Retrieved 10000 activities so far... (offset: 10000)\n",
      "Retrieved 10020 activities so far... (offset: 10020)\n",
      "Retrieved 10040 activities so far... (offset: 10040)\n",
      "Retrieved 10060 activities so far... (offset: 10060)\n",
      "Retrieved 10080 activities so far... (offset: 10080)\n",
      "Retrieved 10100 activities so far... (offset: 10100)\n",
      "Retrieved 10120 activities so far... (offset: 10120)\n",
      "Retrieved 10140 activities so far... (offset: 10140)\n",
      "Retrieved 10160 activities so far... (offset: 10160)\n",
      "Retrieved 10180 activities so far... (offset: 10180)\n",
      "Retrieved 10200 activities so far... (offset: 10200)\n",
      "Retrieved 10220 activities so far... (offset: 10220)\n",
      "Retrieved 10240 activities so far... (offset: 10240)\n",
      "Retrieved 10260 activities so far... (offset: 10260)\n",
      "Retrieved 10280 activities so far... (offset: 10280)\n",
      "Retrieved 10300 activities so far... (offset: 10300)\n",
      "Retrieved 10320 activities so far... (offset: 10320)\n",
      "Retrieved 10340 activities so far... (offset: 10340)\n",
      "Retrieved 10360 activities so far... (offset: 10360)\n",
      "Retrieved 10380 activities so far... (offset: 10380)\n",
      "Retrieved 10400 activities so far... (offset: 10400)\n",
      "Retrieved 10420 activities so far... (offset: 10420)\n",
      "Retrieved 10440 activities so far... (offset: 10440)\n",
      "Retrieved 10460 activities so far... (offset: 10460)\n",
      "Retrieved 10480 activities so far... (offset: 10480)\n",
      "Retrieved 10500 activities so far... (offset: 10500)\n",
      "Retrieved 10520 activities so far... (offset: 10520)\n",
      "Retrieved 10540 activities so far... (offset: 10540)\n",
      "Retrieved 10560 activities so far... (offset: 10560)\n",
      "Retrieved 10580 activities so far... (offset: 10580)\n",
      "Retrieved 10600 activities so far... (offset: 10600)\n",
      "Retrieved 10620 activities so far... (offset: 10620)\n",
      "Retrieved 10640 activities so far... (offset: 10640)\n",
      "Retrieved 10660 activities so far... (offset: 10660)\n",
      "Retrieved 10680 activities so far... (offset: 10680)\n",
      "Retrieved 10700 activities so far... (offset: 10700)\n",
      "Retrieved 10720 activities so far... (offset: 10720)\n",
      "Retrieved 10740 activities so far... (offset: 10740)\n",
      "Retrieved 10760 activities so far... (offset: 10760)\n",
      "Retrieved 10780 activities so far... (offset: 10780)\n",
      "Retrieved 10800 activities so far... (offset: 10800)\n",
      "Retrieved 10820 activities so far... (offset: 10820)\n",
      "Retrieved 10840 activities so far... (offset: 10840)\n",
      "Retrieved 10860 activities so far... (offset: 10860)\n",
      "Retrieved 10880 activities so far... (offset: 10880)\n",
      "Retrieved 10900 activities so far... (offset: 10900)\n",
      "Retrieved 10920 activities so far... (offset: 10920)\n",
      "Retrieved 10940 activities so far... (offset: 10940)\n",
      "Retrieved 10960 activities so far... (offset: 10960)\n",
      "Retrieved 10980 activities so far... (offset: 10980)\n",
      "Retrieved 11000 activities so far... (offset: 11000)\n",
      "Retrieved 11020 activities so far... (offset: 11020)\n",
      "Retrieved 11040 activities so far... (offset: 11040)\n",
      "Retrieved 11060 activities so far... (offset: 11060)\n",
      "Retrieved 11080 activities so far... (offset: 11080)\n",
      "Retrieved 11100 activities so far... (offset: 11100)\n",
      "Retrieved 11120 activities so far... (offset: 11120)\n",
      "Retrieved 11140 activities so far... (offset: 11140)\n",
      "Retrieved 11160 activities so far... (offset: 11160)\n",
      "Retrieved 11180 activities so far... (offset: 11180)\n",
      "Retrieved 11200 activities so far... (offset: 11200)\n",
      "Retrieved 11220 activities so far... (offset: 11220)\n",
      "Retrieved 11240 activities so far... (offset: 11240)\n",
      "Retrieved 11260 activities so far... (offset: 11260)\n",
      "Retrieved 11280 activities so far... (offset: 11280)\n",
      "Retrieved 11300 activities so far... (offset: 11300)\n",
      "Retrieved 11320 activities so far... (offset: 11320)\n",
      "Retrieved 11340 activities so far... (offset: 11340)\n",
      "Retrieved 11360 activities so far... (offset: 11360)\n",
      "Retrieved 11380 activities so far... (offset: 11380)\n",
      "Retrieved 11400 activities so far... (offset: 11400)\n",
      "Retrieved 11420 activities so far... (offset: 11420)\n",
      "Retrieved 11440 activities so far... (offset: 11440)\n",
      "Retrieved 11460 activities so far... (offset: 11460)\n",
      "Retrieved 11480 activities so far... (offset: 11480)\n",
      "Retrieved 11500 activities so far... (offset: 11500)\n",
      "Retrieved 11520 activities so far... (offset: 11520)\n",
      "Retrieved 11540 activities so far... (offset: 11540)\n",
      "Retrieved 11560 activities so far... (offset: 11560)\n",
      "Retrieved 11580 activities so far... (offset: 11580)\n",
      "Retrieved 11600 activities so far... (offset: 11600)\n",
      "Retrieved 11620 activities so far... (offset: 11620)\n",
      "Retrieved 11640 activities so far... (offset: 11640)\n",
      "Retrieved 11660 activities so far... (offset: 11660)\n",
      "Retrieved 11680 activities so far... (offset: 11680)\n",
      "Retrieved 11700 activities so far... (offset: 11700)\n",
      "Retrieved 11720 activities so far... (offset: 11720)\n",
      "Retrieved 11740 activities so far... (offset: 11740)\n",
      "Retrieved 11760 activities so far... (offset: 11760)\n",
      "Retrieved 11780 activities so far... (offset: 11780)\n",
      "Retrieved 11800 activities so far... (offset: 11800)\n",
      "Retrieved 11820 activities so far... (offset: 11820)\n",
      "Retrieved 11840 activities so far... (offset: 11840)\n",
      "Retrieved 11860 activities so far... (offset: 11860)\n",
      "Retrieved 11880 activities so far... (offset: 11880)\n",
      "Retrieved 11900 activities so far... (offset: 11900)\n",
      "Retrieved 11920 activities so far... (offset: 11920)\n",
      "Retrieved 11940 activities so far... (offset: 11940)\n",
      "Retrieved 11960 activities so far... (offset: 11960)\n",
      "Retrieved 11980 activities so far... (offset: 11980)\n",
      "Retrieved 12000 activities so far... (offset: 12000)\n",
      "Retrieved 12020 activities so far... (offset: 12020)\n",
      "Retrieved 12040 activities so far... (offset: 12040)\n",
      "Retrieved 12060 activities so far... (offset: 12060)\n",
      "Retrieved 12080 activities so far... (offset: 12080)\n",
      "Retrieved 12100 activities so far... (offset: 12100)\n",
      "Retrieved 12120 activities so far... (offset: 12120)\n",
      "Retrieved 12140 activities so far... (offset: 12140)\n",
      "Retrieved 12160 activities so far... (offset: 12160)\n",
      "Retrieved 12180 activities so far... (offset: 12180)\n",
      "Retrieved 12200 activities so far... (offset: 12200)\n",
      "Retrieved 12220 activities so far... (offset: 12220)\n",
      "Retrieved 12240 activities so far... (offset: 12240)\n",
      "Retrieved 12260 activities so far... (offset: 12260)\n",
      "Retrieved 12280 activities so far... (offset: 12280)\n",
      "Retrieved 12300 activities so far... (offset: 12300)\n",
      "Retrieved 12320 activities so far... (offset: 12320)\n",
      "Retrieved 12340 activities so far... (offset: 12340)\n",
      "Retrieved 12360 activities so far... (offset: 12360)\n",
      "Retrieved 12380 activities so far... (offset: 12380)\n",
      "Retrieved 12400 activities so far... (offset: 12400)\n",
      "Retrieved 12420 activities so far... (offset: 12420)\n",
      "Retrieved 12440 activities so far... (offset: 12440)\n",
      "Retrieved 12460 activities so far... (offset: 12460)\n",
      "Retrieved 12480 activities so far... (offset: 12480)\n",
      "Retrieved 12500 activities so far... (offset: 12500)\n",
      "Retrieved 12520 activities so far... (offset: 12520)\n",
      "Retrieved 12540 activities so far... (offset: 12540)\n",
      "Retrieved 12560 activities so far... (offset: 12560)\n",
      "Retrieved 12580 activities so far... (offset: 12580)\n",
      "Retrieved 12600 activities so far... (offset: 12600)\n",
      "Retrieved 12620 activities so far... (offset: 12620)\n",
      "Retrieved 12640 activities so far... (offset: 12640)\n",
      "Retrieved 12660 activities so far... (offset: 12660)\n",
      "Retrieved 12680 activities so far... (offset: 12680)\n",
      "Retrieved 12700 activities so far... (offset: 12700)\n",
      "Retrieved 12720 activities so far... (offset: 12720)\n",
      "Retrieved 12740 activities so far... (offset: 12740)\n",
      "Retrieved 12760 activities so far... (offset: 12760)\n",
      "Retrieved 12780 activities so far... (offset: 12780)\n",
      "Retrieved 12800 activities so far... (offset: 12800)\n",
      "Retrieved 12820 activities so far... (offset: 12820)\n",
      "Retrieved 12840 activities so far... (offset: 12840)\n",
      "Retrieved 12860 activities so far... (offset: 12860)\n",
      "Retrieved 12880 activities so far... (offset: 12880)\n",
      "Retrieved 12900 activities so far... (offset: 12900)\n",
      "Retrieved 12920 activities so far... (offset: 12920)\n",
      "Retrieved 12940 activities so far... (offset: 12940)\n",
      "Retrieved 12960 activities so far... (offset: 12960)\n",
      "Retrieved 12980 activities so far... (offset: 12980)\n",
      "Retrieved 13000 activities so far... (offset: 13000)\n",
      "Retrieved 13020 activities so far... (offset: 13020)\n",
      "Retrieved 13040 activities so far... (offset: 13040)\n",
      "Retrieved 13060 activities so far... (offset: 13060)\n",
      "Retrieved 13080 activities so far... (offset: 13080)\n",
      "Retrieved 13100 activities so far... (offset: 13100)\n",
      "Retrieved 13120 activities so far... (offset: 13120)\n",
      "Retrieved 13140 activities so far... (offset: 13140)\n",
      "Retrieved 13160 activities so far... (offset: 13160)\n",
      "Retrieved 13180 activities so far... (offset: 13180)\n",
      "Retrieved 13200 activities so far... (offset: 13200)\n",
      "Retrieved 13220 activities so far... (offset: 13220)\n",
      "Retrieved 13240 activities so far... (offset: 13240)\n",
      "Retrieved 13260 activities so far... (offset: 13260)\n",
      "Retrieved 13280 activities so far... (offset: 13280)\n",
      "Retrieved 13300 activities so far... (offset: 13300)\n",
      "Retrieved 13320 activities so far... (offset: 13320)\n",
      "Retrieved 13340 activities so far... (offset: 13340)\n",
      "Retrieved 13360 activities so far... (offset: 13360)\n",
      "Retrieved 13380 activities so far... (offset: 13380)\n",
      "Retrieved 13400 activities so far... (offset: 13400)\n",
      "Retrieved 13420 activities so far... (offset: 13420)\n",
      "Retrieved 13440 activities so far... (offset: 13440)\n",
      "Retrieved 13460 activities so far... (offset: 13460)\n",
      "Retrieved 13480 activities so far... (offset: 13480)\n",
      "Retrieved 13500 activities so far... (offset: 13500)\n",
      "Retrieved 13520 activities so far... (offset: 13520)\n",
      "Retrieved 13540 activities so far... (offset: 13540)\n",
      "Retrieved 13560 activities so far... (offset: 13560)\n",
      "Retrieved 13580 activities so far... (offset: 13580)\n",
      "Retrieved 13600 activities so far... (offset: 13600)\n",
      "Retrieved 13620 activities so far... (offset: 13620)\n",
      "Retrieved 13640 activities so far... (offset: 13640)\n",
      "Retrieved 13660 activities so far... (offset: 13660)\n",
      "Retrieved 13680 activities so far... (offset: 13680)\n",
      "Retrieved 13700 activities so far... (offset: 13700)\n",
      "Retrieved 13720 activities so far... (offset: 13720)\n",
      "Retrieved 13740 activities so far... (offset: 13740)\n",
      "Retrieved 13760 activities so far... (offset: 13760)\n",
      "Retrieved 13780 activities so far... (offset: 13780)\n",
      "Retrieved 13800 activities so far... (offset: 13800)\n",
      "Retrieved 13820 activities so far... (offset: 13820)\n",
      "Retrieved 13840 activities so far... (offset: 13840)\n",
      "Retrieved 13860 activities so far... (offset: 13860)\n",
      "Retrieved 13880 activities so far... (offset: 13880)\n",
      "Retrieved 13900 activities so far... (offset: 13900)\n",
      "Retrieved 13920 activities so far... (offset: 13920)\n",
      "Retrieved 13940 activities so far... (offset: 13940)\n",
      "Retrieved 13960 activities so far... (offset: 13960)\n",
      "Retrieved 13980 activities so far... (offset: 13980)\n",
      "Retrieved 14000 activities so far... (offset: 14000)\n",
      "Retrieved 14020 activities so far... (offset: 14020)\n",
      "Retrieved 14040 activities so far... (offset: 14040)\n",
      "Retrieved 14060 activities so far... (offset: 14060)\n",
      "Retrieved 14080 activities so far... (offset: 14080)\n",
      "Retrieved 14100 activities so far... (offset: 14100)\n",
      "Retrieved 14120 activities so far... (offset: 14120)\n",
      "Retrieved 14140 activities so far... (offset: 14140)\n",
      "Retrieved 14160 activities so far... (offset: 14160)\n",
      "Retrieved 14180 activities so far... (offset: 14180)\n",
      "Retrieved 14200 activities so far... (offset: 14200)\n",
      "Retrieved 14220 activities so far... (offset: 14220)\n",
      "Retrieved 14240 activities so far... (offset: 14240)\n",
      "Retrieved 14260 activities so far... (offset: 14260)\n",
      "Retrieved 14280 activities so far... (offset: 14280)\n",
      "Retrieved 14300 activities so far... (offset: 14300)\n",
      "Retrieved 14320 activities so far... (offset: 14320)\n",
      "Retrieved 14340 activities so far... (offset: 14340)\n",
      "Retrieved 14360 activities so far... (offset: 14360)\n",
      "Retrieved 14380 activities so far... (offset: 14380)\n",
      "Retrieved 14400 activities so far... (offset: 14400)\n",
      "Retrieved 14420 activities so far... (offset: 14420)\n",
      "Retrieved 14440 activities so far... (offset: 14440)\n",
      "Retrieved 14460 activities so far... (offset: 14460)\n",
      "Retrieved 14480 activities so far... (offset: 14480)\n",
      "Retrieved 14500 activities so far... (offset: 14500)\n",
      "Retrieved 14520 activities so far... (offset: 14520)\n",
      "Retrieved 14540 activities so far... (offset: 14540)\n",
      "Retrieved 14560 activities so far... (offset: 14560)\n",
      "Retrieved 14580 activities so far... (offset: 14580)\n",
      "Retrieved 14600 activities so far... (offset: 14600)\n",
      "Retrieved 14620 activities so far... (offset: 14620)\n",
      "Retrieved 14640 activities so far... (offset: 14640)\n",
      "Retrieved 14660 activities so far... (offset: 14660)\n",
      "Retrieved 14680 activities so far... (offset: 14680)\n",
      "Retrieved 14700 activities so far... (offset: 14700)\n",
      "Retrieved 14720 activities so far... (offset: 14720)\n",
      "Retrieved 14740 activities so far... (offset: 14740)\n",
      "Retrieved 14760 activities so far... (offset: 14760)\n",
      "Retrieved 14780 activities so far... (offset: 14780)\n",
      "Retrieved 14800 activities so far... (offset: 14800)\n",
      "Retrieved 14820 activities so far... (offset: 14820)\n",
      "Retrieved 14840 activities so far... (offset: 14840)\n",
      "Retrieved 14860 activities so far... (offset: 14860)\n",
      "Retrieved 14880 activities so far... (offset: 14880)\n",
      "Retrieved 14900 activities so far... (offset: 14900)\n",
      "Retrieved 14920 activities so far... (offset: 14920)\n",
      "Retrieved 14940 activities so far... (offset: 14940)\n",
      "Retrieved 14960 activities so far... (offset: 14960)\n",
      "Retrieved 14980 activities so far... (offset: 14980)\n",
      "Retrieved 15000 activities so far... (offset: 15000)\n",
      "Retrieved 15020 activities so far... (offset: 15020)\n",
      "Retrieved 15040 activities so far... (offset: 15040)\n",
      "Retrieved 15060 activities so far... (offset: 15060)\n",
      "Retrieved 15080 activities so far... (offset: 15080)\n",
      "Retrieved 15100 activities so far... (offset: 15100)\n",
      "Retrieved 15120 activities so far... (offset: 15120)\n",
      "Retrieved 15140 activities so far... (offset: 15140)\n",
      "Retrieved 15160 activities so far... (offset: 15160)\n",
      "Retrieved 15180 activities so far... (offset: 15180)\n",
      "Retrieved 15200 activities so far... (offset: 15200)\n",
      "Retrieved 15220 activities so far... (offset: 15220)\n",
      "Retrieved 15240 activities so far... (offset: 15240)\n",
      "Retrieved 15260 activities so far... (offset: 15260)\n",
      "Retrieved 15280 activities so far... (offset: 15280)\n",
      "Retrieved 15300 activities so far... (offset: 15300)\n",
      "Retrieved 15320 activities so far... (offset: 15320)\n",
      "Retrieved 15340 activities so far... (offset: 15340)\n",
      "Retrieved 15360 activities so far... (offset: 15360)\n",
      "Retrieved 15380 activities so far... (offset: 15380)\n",
      "Retrieved 15400 activities so far... (offset: 15400)\n",
      "Retrieved 15420 activities so far... (offset: 15420)\n",
      "Retrieved 15440 activities so far... (offset: 15440)\n",
      "Retrieved 15460 activities so far... (offset: 15460)\n",
      "Retrieved 15480 activities so far... (offset: 15480)\n",
      "Retrieved 15500 activities so far... (offset: 15500)\n",
      "Retrieved 15520 activities so far... (offset: 15520)\n",
      "Retrieved 15540 activities so far... (offset: 15540)\n",
      "Retrieved 15560 activities so far... (offset: 15560)\n",
      "Retrieved 15580 activities so far... (offset: 15580)\n",
      "Retrieved 15600 activities so far... (offset: 15600)\n",
      "Retrieved 15620 activities so far... (offset: 15620)\n",
      "Retrieved 15640 activities so far... (offset: 15640)\n",
      "Retrieved 15660 activities so far... (offset: 15660)\n",
      "Retrieved 15680 activities so far... (offset: 15680)\n",
      "Retrieved 15700 activities so far... (offset: 15700)\n",
      "Retrieved 15720 activities so far... (offset: 15720)\n",
      "Retrieved 15740 activities so far... (offset: 15740)\n",
      "Retrieved 15760 activities so far... (offset: 15760)\n",
      "Retrieved 15780 activities so far... (offset: 15780)\n",
      "Retrieved 15800 activities so far... (offset: 15800)\n",
      "Retrieved 15820 activities so far... (offset: 15820)\n",
      "Retrieved 15840 activities so far... (offset: 15840)\n",
      "Retrieved 15860 activities so far... (offset: 15860)\n",
      "Retrieved 15880 activities so far... (offset: 15880)\n",
      "Retrieved 15900 activities so far... (offset: 15900)\n",
      "Retrieved 15920 activities so far... (offset: 15920)\n",
      "Retrieved 15940 activities so far... (offset: 15940)\n",
      "Retrieved 15960 activities so far... (offset: 15960)\n",
      "Retrieved 15980 activities so far... (offset: 15980)\n",
      "Retrieved 16000 activities so far... (offset: 16000)\n",
      "Retrieved 16020 activities so far... (offset: 16020)\n",
      "Retrieved 16040 activities so far... (offset: 16040)\n",
      "Retrieved 16060 activities so far... (offset: 16060)\n",
      "Retrieved 16080 activities so far... (offset: 16080)\n",
      "Retrieved 16100 activities so far... (offset: 16100)\n",
      "Retrieved 16120 activities so far... (offset: 16120)\n",
      "Retrieved 16140 activities so far... (offset: 16140)\n",
      "Retrieved 16160 activities so far... (offset: 16160)\n",
      "Retrieved 16180 activities so far... (offset: 16180)\n",
      "Retrieved 16200 activities so far... (offset: 16200)\n",
      "Retrieved 16220 activities so far... (offset: 16220)\n",
      "Retrieved 16240 activities so far... (offset: 16240)\n",
      "Retrieved 16260 activities so far... (offset: 16260)\n",
      "Retrieved 16280 activities so far... (offset: 16280)\n",
      "Retrieved 16300 activities so far... (offset: 16300)\n",
      "Retrieved 16320 activities so far... (offset: 16320)\n",
      "Retrieved 16340 activities so far... (offset: 16340)\n",
      "Retrieved 16360 activities so far... (offset: 16360)\n",
      "Retrieved 16380 activities so far... (offset: 16380)\n",
      "Retrieved 16400 activities so far... (offset: 16400)\n",
      "Retrieved 16420 activities so far... (offset: 16420)\n",
      "Retrieved 16440 activities so far... (offset: 16440)\n",
      "Retrieved 16460 activities so far... (offset: 16460)\n",
      "Retrieved 16480 activities so far... (offset: 16480)\n",
      "Retrieved 16500 activities so far... (offset: 16500)\n",
      "Retrieved 16520 activities so far... (offset: 16520)\n",
      "Retrieved 16540 activities so far... (offset: 16540)\n",
      "Retrieved 16560 activities so far... (offset: 16560)\n",
      "Retrieved 16580 activities so far... (offset: 16580)\n",
      "Retrieved 16600 activities so far... (offset: 16600)\n",
      "Retrieved 16620 activities so far... (offset: 16620)\n",
      "Retrieved 16640 activities so far... (offset: 16640)\n",
      "Retrieved 16660 activities so far... (offset: 16660)\n",
      "Retrieved 16680 activities so far... (offset: 16680)\n",
      "Retrieved 16700 activities so far... (offset: 16700)\n",
      "Retrieved 16720 activities so far... (offset: 16720)\n",
      "Retrieved 16740 activities so far... (offset: 16740)\n",
      "Retrieved 16760 activities so far... (offset: 16760)\n",
      "Retrieved 16780 activities so far... (offset: 16780)\n",
      "Retrieved 16800 activities so far... (offset: 16800)\n",
      "Retrieved 16820 activities so far... (offset: 16820)\n",
      "Retrieved 16840 activities so far... (offset: 16840)\n",
      "Retrieved 16860 activities so far... (offset: 16860)\n",
      "Retrieved 16880 activities so far... (offset: 16880)\n",
      "Retrieved 16900 activities so far... (offset: 16900)\n",
      "Retrieved 16920 activities so far... (offset: 16920)\n",
      "Retrieved 16940 activities so far... (offset: 16940)\n",
      "Retrieved 16960 activities so far... (offset: 16960)\n",
      "Retrieved 16980 activities so far... (offset: 16980)\n",
      "Retrieved 17000 activities so far... (offset: 17000)\n",
      "Retrieved 17020 activities so far... (offset: 17020)\n",
      "Retrieved 17040 activities so far... (offset: 17040)\n",
      "Retrieved 17060 activities so far... (offset: 17060)\n",
      "Retrieved 17080 activities so far... (offset: 17080)\n",
      "Retrieved 17100 activities so far... (offset: 17100)\n",
      "Retrieved 17120 activities so far... (offset: 17120)\n",
      "Retrieved 17140 activities so far... (offset: 17140)\n",
      "Retrieved 17160 activities so far... (offset: 17160)\n",
      "Retrieved 17180 activities so far... (offset: 17180)\n",
      "Retrieved 17200 activities so far... (offset: 17200)\n",
      "Retrieved 17220 activities so far... (offset: 17220)\n",
      "Retrieved 17240 activities so far... (offset: 17240)\n",
      "Retrieved 17260 activities so far... (offset: 17260)\n",
      "Retrieved 17280 activities so far... (offset: 17280)\n",
      "Retrieved 17300 activities so far... (offset: 17300)\n",
      "Retrieved 17320 activities so far... (offset: 17320)\n",
      "Retrieved 17340 activities so far... (offset: 17340)\n",
      "Retrieved 17360 activities so far... (offset: 17360)\n",
      "Retrieved 17380 activities so far... (offset: 17380)\n",
      "Retrieved 17400 activities so far... (offset: 17400)\n",
      "Retrieved 17420 activities so far... (offset: 17420)\n",
      "Retrieved 17440 activities so far... (offset: 17440)\n",
      "Retrieved 17460 activities so far... (offset: 17460)\n",
      "Retrieved 17480 activities so far... (offset: 17480)\n",
      "Retrieved 17500 activities so far... (offset: 17500)\n",
      "Retrieved 17520 activities so far... (offset: 17520)\n",
      "Retrieved 17540 activities so far... (offset: 17540)\n",
      "Retrieved 17560 activities so far... (offset: 17560)\n",
      "Retrieved 17580 activities so far... (offset: 17580)\n",
      "Retrieved 17600 activities so far... (offset: 17600)\n",
      "Retrieved 17620 activities so far... (offset: 17620)\n",
      "Retrieved 17640 activities so far... (offset: 17640)\n",
      "Retrieved 17660 activities so far... (offset: 17660)\n",
      "Retrieved 17680 activities so far... (offset: 17680)\n",
      "Retrieved 17700 activities so far... (offset: 17700)\n",
      "Retrieved 17720 activities so far... (offset: 17720)\n",
      "Retrieved 17740 activities so far... (offset: 17740)\n",
      "Retrieved 17760 activities so far... (offset: 17760)\n",
      "Retrieved 17780 activities so far... (offset: 17780)\n",
      "Retrieved 17800 activities so far... (offset: 17800)\n",
      "Retrieved 17820 activities so far... (offset: 17820)\n",
      "Retrieved 17840 activities so far... (offset: 17840)\n",
      "Retrieved 17860 activities so far... (offset: 17860)\n",
      "Retrieved 17880 activities so far... (offset: 17880)\n",
      "Retrieved 17900 activities so far... (offset: 17900)\n",
      "Retrieved 17920 activities so far... (offset: 17920)\n",
      "Retrieved 17940 activities so far... (offset: 17940)\n",
      "Retrieved 17960 activities so far... (offset: 17960)\n",
      "Retrieved 17980 activities so far... (offset: 17980)\n",
      "Retrieved 18000 activities so far... (offset: 18000)\n",
      "Retrieved 18020 activities so far... (offset: 18020)\n",
      "Retrieved 18040 activities so far... (offset: 18040)\n",
      "Retrieved 18060 activities so far... (offset: 18060)\n",
      "Retrieved 18080 activities so far... (offset: 18080)\n",
      "Retrieved 18100 activities so far... (offset: 18100)\n",
      "Retrieved 18120 activities so far... (offset: 18120)\n",
      "Retrieved 18140 activities so far... (offset: 18140)\n",
      "Retrieved 18160 activities so far... (offset: 18160)\n",
      "Retrieved 18180 activities so far... (offset: 18180)\n",
      "Retrieved 18200 activities so far... (offset: 18200)\n",
      "Retrieved 18220 activities so far... (offset: 18220)\n",
      "Retrieved 18240 activities so far... (offset: 18240)\n",
      "Retrieved 18260 activities so far... (offset: 18260)\n",
      "Retrieved 18280 activities so far... (offset: 18280)\n",
      "Retrieved 18300 activities so far... (offset: 18300)\n",
      "Retrieved 18320 activities so far... (offset: 18320)\n",
      "Retrieved 18340 activities so far... (offset: 18340)\n",
      "Retrieved 18360 activities so far... (offset: 18360)\n",
      "Retrieved 18380 activities so far... (offset: 18380)\n",
      "Retrieved 18400 activities so far... (offset: 18400)\n",
      "Retrieved 18420 activities so far... (offset: 18420)\n",
      "Retrieved 18440 activities so far... (offset: 18440)\n",
      "Retrieved 18460 activities so far... (offset: 18460)\n",
      "Retrieved 18480 activities so far... (offset: 18480)\n",
      "Retrieved 18500 activities so far... (offset: 18500)\n",
      "Retrieved 18520 activities so far... (offset: 18520)\n",
      "Retrieved 18540 activities so far... (offset: 18540)\n",
      "Retrieved 18560 activities so far... (offset: 18560)\n",
      "Retrieved 18580 activities so far... (offset: 18580)\n",
      "Retrieved 18600 activities so far... (offset: 18600)\n",
      "Retrieved 18620 activities so far... (offset: 18620)\n",
      "Retrieved 18640 activities so far... (offset: 18640)\n",
      "Retrieved 18660 activities so far... (offset: 18660)\n",
      "Retrieved 18680 activities so far... (offset: 18680)\n",
      "Retrieved 18700 activities so far... (offset: 18700)\n",
      "Retrieved 18720 activities so far... (offset: 18720)\n",
      "Retrieved 18740 activities so far... (offset: 18740)\n",
      "Retrieved 18760 activities so far... (offset: 18760)\n",
      "Retrieved 18780 activities so far... (offset: 18780)\n",
      "Retrieved 18800 activities so far... (offset: 18800)\n",
      "Retrieved 18820 activities so far... (offset: 18820)\n",
      "Retrieved 18840 activities so far... (offset: 18840)\n",
      "Retrieved 18860 activities so far... (offset: 18860)\n",
      "Retrieved 18880 activities so far... (offset: 18880)\n",
      "Retrieved 18900 activities so far... (offset: 18900)\n",
      "Retrieved 18920 activities so far... (offset: 18920)\n",
      "Retrieved 18940 activities so far... (offset: 18940)\n",
      "Retrieved 18946 activities so far... (offset: 18946)\n",
      "Total activities retrieved: 18946\n",
      "Initial DataFrame shape: (18946, 8)\n",
      "Shape after filtering activity types: (11644, 8)\n",
      "Shape after dropping missing pChEMBL values: (7549, 8)\n",
      "Fetching molecular structures...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching molecular structures:  21%|                                   | 1197/5703 [16:30<1:04:05,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No SMILES found for CHEMBL2448138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching molecular structures:  80%|        | 4585/5703 [1:02:11<4:21:42, 14.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching SMILES for CHEMBL4793341: HTTPSConnectionPool(host='www.ebi.ac.uk', port=443): Max retries exceeded with url: /chembl/api/data/molecule/CHEMBL4793341 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001E4BC357490>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching molecular structures:  80%|        | 4586/5703 [1:02:34<5:11:54, 16.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching SMILES for CHEMBL4760651: HTTPSConnectionPool(host='www.ebi.ac.uk', port=443): Max retries exceeded with url: /chembl/api/data/molecule/CHEMBL4760651 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001E4B450AB10>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching molecular structures:  83%|       | 4742/5703 [1:11:41<8:18:33, 31.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching SMILES for CHEMBL4764003: HTTPSConnectionPool(host='www.ebi.ac.uk', port=443): Max retries exceeded with url: /chembl/api/data/molecule/CHEMBL4764003 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001E4B4524D50>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching molecular structures: 100%|| 5703/5703 [1:30:31<00:00,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES DataFrame shape: (5699, 2)\n",
      "Final merged DataFrame shape: (7549, 9)\n",
      "Dataset saved as AChE_ligands.csv\n",
      "  molecule_chembl_id  pchembl_value standard_type standard_units  \\\n",
      "0       CHEMBL133897           6.12          IC50             nM   \n",
      "1       CHEMBL336398           7.00          IC50             nM   \n",
      "2       CHEMBL130628           6.52          IC50             nM   \n",
      "3       CHEMBL130478           6.10          IC50             nM   \n",
      "4       CHEMBL130112           5.62          IC50             nM   \n",
      "\n",
      "   standard_value  type units value  \\\n",
      "0           750.0  IC50    uM  0.75   \n",
      "1           100.0  IC50    uM   0.1   \n",
      "2           300.0  IC50    uM   0.3   \n",
      "3           800.0  IC50    uM   0.8   \n",
      "4          2400.0  IC50    uM   2.4   \n",
      "\n",
      "                                          SMILES  \n",
      "0          CCOc1nn(-c2cccc(OCc3ccccc3)c2)c(=O)o1  \n",
      "1     O=C(N1CCCCC1)n1nc(-c2ccc(Cl)cc2)nc1SCC1CC1  \n",
      "2  O=C(N1CCCCC1)n1nc(-c2ccc(Cl)cc2)nc1SCC(F)(F)F  \n",
      "3      CSc1nc(-c2ccc(OC(F)(F)F)cc2)nn1C(=O)N(C)C  \n",
      "4       CSc1nc(-c2ccc(C)cc2)nn1C(=O)N(C)c1ccccc1  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from chembl_webresource_client.new_client import new_client\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def fetch_ache_ligands():\n",
    "    target_client = new_client.target\n",
    "    activity_client = new_client.activity\n",
    "    \n",
    "    print(\"Searching for Acetylcholinesterase (AChE) target...\")\n",
    "    target = target_client.search('P22303')\n",
    "    if not target:\n",
    "        print(\"No target found!\")\n",
    "        return None\n",
    "    \n",
    "    target_id = target[0]['target_chembl_id']\n",
    "    print(f\"Found target: {target_id}\")\n",
    "    \n",
    "    print(\"Fetching ligand activities...\")\n",
    "    activities = []\n",
    "    offset = 0\n",
    "    chunk_size = 2000  # Increased chunk size for faster retrieval\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            chunk = list(activity_client.filter(target_chembl_id=target_id).only(\n",
    "                ['molecule_chembl_id', 'standard_type', 'standard_value', 'standard_units', 'pchembl_value']\n",
    "            )[offset:offset + chunk_size])\n",
    "            \n",
    "            if not chunk:\n",
    "                break  # Stop when no more data is retrieved\n",
    "            \n",
    "            activities.extend(chunk)\n",
    "            offset += len(chunk)  # Adjust offset dynamically\n",
    "            print(f\"Retrieved {len(activities)} activities so far... (offset: {offset})\")\n",
    "            time.sleep(0.5)  # Reduced delay to optimize retrieval speed\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching activities at offset {offset}: {e}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"Total activities retrieved: {len(activities)}\")\n",
    "    df = pd.DataFrame(activities)\n",
    "    print(f\"Initial DataFrame shape: {df.shape}\")\n",
    "    \n",
    "    # Keep only activity types relevant to binding\n",
    "    df = df[df['standard_type'].isin(['IC50', 'Ki', 'Kd'])]\n",
    "    print(f\"Shape after filtering activity types: {df.shape}\")\n",
    "    \n",
    "    # Convert numeric columns\n",
    "    df['standard_value'] = pd.to_numeric(df['standard_value'], errors='coerce')\n",
    "    df['pchembl_value'] = pd.to_numeric(df['pchembl_value'], errors='coerce')\n",
    "    \n",
    "    # Drop missing values\n",
    "    df_cleaned = df.dropna(subset=['pchembl_value'])\n",
    "    print(f\"Shape after dropping missing pChEMBL values: {df_cleaned.shape}\")\n",
    "    \n",
    "    # Fetch molecular structures (SMILES) with progress bar\n",
    "    molecule_client = new_client.molecule\n",
    "    smiles_data = []\n",
    "    print(\"Fetching molecular structures...\")\n",
    "    for chembl_id in tqdm(df_cleaned['molecule_chembl_id'].unique(), desc=\"Fetching molecular structures\"):\n",
    "        try:\n",
    "            mol = molecule_client.get(chembl_id)\n",
    "            if 'molecule_structures' in mol and mol['molecule_structures']:\n",
    "                smiles_data.append((chembl_id, mol['molecule_structures']['canonical_smiles']))\n",
    "            else:\n",
    "                print(f\"No SMILES found for {chembl_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching SMILES for {chembl_id}: {e}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    smiles_df = pd.DataFrame(smiles_data, columns=['molecule_chembl_id', 'SMILES'])\n",
    "    print(f\"SMILES DataFrame shape: {smiles_df.shape}\")\n",
    "    \n",
    "    df_final = df_cleaned.merge(smiles_df, on='molecule_chembl_id', how='left')\n",
    "    print(f\"Final merged DataFrame shape: {df_final.shape}\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    df_final.to_csv('AChE_ligands.csv', index=False)\n",
    "    print(\"Dataset saved as AChE_ligands.csv\")\n",
    "    return df_final\n",
    "\n",
    "# Fetch and save dataset\n",
    "dataset = fetch_ache_ligands()\n",
    "if dataset is not None:\n",
    "    print(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f7c4c0-e293-43b4-8f1b-08824c2aecd1",
   "metadata": {},
   "source": [
    "# removing unnecessary columns from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "379e8a65-4a87-4e2a-b042-04eca197663e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved as C:\\Users\\ngaga\\OneDrive\\Desktop\\aragen\\AChE_ligands_cleaned.csv\n",
      "  molecule_chembl_id  pchembl_value standard_type standard_units  \\\n",
      "0       CHEMBL133897           6.12          IC50             nM   \n",
      "1       CHEMBL336398           7.00          IC50             nM   \n",
      "2       CHEMBL130628           6.52          IC50             nM   \n",
      "3       CHEMBL130478           6.10          IC50             nM   \n",
      "4       CHEMBL130112           5.62          IC50             nM   \n",
      "\n",
      "   standard_value                                         SMILES  \n",
      "0           750.0          CCOc1nn(-c2cccc(OCc3ccccc3)c2)c(=O)o1  \n",
      "1           100.0     O=C(N1CCCCC1)n1nc(-c2ccc(Cl)cc2)nc1SCC1CC1  \n",
      "2           300.0  O=C(N1CCCCC1)n1nc(-c2ccc(Cl)cc2)nc1SCC(F)(F)F  \n",
      "3           800.0      CSc1nc(-c2ccc(OC(F)(F)F)cc2)nn1C(=O)N(C)C  \n",
      "4          2400.0       CSc1nc(-c2ccc(C)cc2)nn1C(=O)N(C)c1ccccc1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\aragen\\AChE_ligands_csv.csv\"  # Update with actual path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop the unnecessary columns\n",
    "columns_to_drop = ['type', 'units', 'value']\n",
    "df.drop(columns=columns_to_drop, inplace=True, errors='ignore')  # Ignore errors if columns don't exist\n",
    "\n",
    "# Save the cleaned dataset back to a CSV file\n",
    "cleaned_file_path = r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\aragen\\AChE_ligands_cleaned.csv\"\n",
    "df.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved as {cleaned_file_path}\")\n",
    "print(df.head())  # Preview the cleaned dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6af8715-85dd-46e8-85bb-701b34651b6c",
   "metadata": {},
   "source": [
    "# determining if a ligand is active or inactive based on activity score(70%-pCHEMBL value + 30%-standard value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a1ab85a-73e7-4b70-80ef-effd14b1cffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset saved as C:\\Users\\ngaga\\OneDrive\\Desktop\\aragen\\AChE_ligands_weighted_activity.csv\n",
      "Activity\n",
      "Inactive    3819\n",
      "Active      3730\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\aragen\\AChE_ligands_cleaned.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure numeric columns\n",
    "df['pchembl_value'] = pd.to_numeric(df['pchembl_value'], errors='coerce')\n",
    "df['standard_value'] = pd.to_numeric(df['standard_value'], errors='coerce')\n",
    "\n",
    "# Normalization function for pChEMBL value (scaling between 0 and 1)\n",
    "def normalize_pchembl(pchembl):\n",
    "    return min(max((pchembl - 4) / 4, 0), 1) if not np.isnan(pchembl) else 0\n",
    "\n",
    "# Normalization function for Standard Value (inverse scale, lower is better)\n",
    "def normalize_standard_value(value):\n",
    "    return min(max(1 - (value / 1000), 0), 1) if not np.isnan(value) else 0\n",
    "\n",
    "# Compute Activity Score\n",
    "df['pchembl_score'] = df['pchembl_value'].apply(normalize_pchembl)\n",
    "df['standard_score'] = df.apply(lambda row: normalize_standard_value(row['standard_value']) \n",
    "                                if row['standard_type'] in ['IC50', 'Ki', 'Kd'] else 0, axis=1)\n",
    "\n",
    "df['Activity_Score'] = 0.7 * df['pchembl_score'] + 0.3 * df['standard_score']\n",
    "\n",
    "# Classify as Active or Inactive\n",
    "df['Activity'] = df['Activity_Score'].apply(lambda x: 'Active' if x >= 0.5 else 'Inactive')\n",
    "\n",
    "# Save the updated dataset\n",
    "updated_file_path = r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\aragen\\AChE_ligands_weighted_activity.csv\"\n",
    "df.to_csv(updated_file_path, index=False)\n",
    "\n",
    "print(f\"Updated dataset saved as {updated_file_path}\")\n",
    "print(df['Activity'].value_counts())  # Print Active/Inactive counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169febd3-52a9-48f6-85b3-4ab76fae1dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9762421c-ea36-4d3c-a951-88cb0d36c997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5ddff15-2179-4091-a28b-b765cbf13853",
   "metadata": {},
   "source": [
    "# with xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d60460aa-27e0-4fe6-983f-58c719743c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8397\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.87      0.85       764\n",
      "           1       0.86      0.80      0.83       746\n",
      "\n",
      "    accuracy                           0.84      1510\n",
      "   macro avg       0.84      0.84      0.84      1510\n",
      "weighted avg       0.84      0.84      0.84      1510\n",
      "\n",
      "Model saved as ache_ligand_classifier.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\aragen\\AChE_ligands_weighted_activity.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure SMILES column is clean and contains valid strings\n",
    "df['SMILES'] = df['SMILES'].replace(np.nan, '').replace('nan', '').astype(str)\n",
    "\n",
    "# Convert SMILES to Morgan Fingerprints (2048-bit vector)\n",
    "def smiles_to_fingerprint(smiles):\n",
    "    if not smiles or smiles.strip() == '':  # Handle empty SMILES\n",
    "        return [0] * 2048  # Return a zero vector\n",
    "\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        return list(AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048))\n",
    "    else:\n",
    "        return [0] * 2048  # Return zero vector if conversion fails\n",
    "\n",
    "# Generate fingerprints\n",
    "df['Fingerprint'] = df['SMILES'].apply(smiles_to_fingerprint)\n",
    "\n",
    "# Convert fingerprint list into separate columns (feature matrix)\n",
    "X = np.array(df['Fingerprint'].tolist())\n",
    "\n",
    "# Target variable (Active = 1, Inactive = 0)\n",
    "y = (df['Activity'] == 'Active').astype(int)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train XGBoost model\n",
    "model = XGBClassifier(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, \"ache_ligand_classifier.pkl\")\n",
    "print(\"Model saved as ache_ligand_classifier.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f5b74e-4935-440c-bff6-7588a2fa6296",
   "metadata": {},
   "source": [
    "# multiple models testing for better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8274623d-c666-4838-8491-1b20dfe9d488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Logistic Regression...\n",
      "Logistic Regression Accuracy: 0.8489\n",
      "\n",
      "Training Random Forest...\n",
      "Random Forest Accuracy: 0.7939\n",
      "\n",
      "Training XGBoost...\n",
      "XGBoost Accuracy: 0.8588\n",
      "\n",
      "Training LightGBM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngaga\\AppData\\Roaming\\Python\\Python311\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"C:\\Users\\ngaga\\AppData\\Roaming\\Python\\Python311\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2980, number of negative: 3055\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.104649 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2912\n",
      "[LightGBM] [Info] Number of data points in the train set: 6035, number of used features: 1456\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493786 -> initscore=-0.024856\n",
      "[LightGBM] [Info] Start training from score -0.024856\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM Accuracy: 0.8423\n",
      "\n",
      "Training SVM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngaga\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.8549\n",
      "\n",
      "Training MLP (Neural Network)...\n",
      "MLP (Neural Network) Accuracy: 0.8569\n",
      "\n",
      "Best model: XGBoost with accuracy: 0.8588\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\aragen\\AChE_ligands_weighted_activity.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert SMILES to Morgan Fingerprints (2048-bit vector)\n",
    "def smiles_to_fingerprint(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        return list(AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048))\n",
    "    else:\n",
    "        return [0] * 2048  # Return zero vector if conversion fails\n",
    "\n",
    "# Drop NaN SMILES values\n",
    "df = df.dropna(subset=['SMILES'])\n",
    "\n",
    "# Generate fingerprints\n",
    "df['Fingerprint'] = df['SMILES'].apply(smiles_to_fingerprint)\n",
    "\n",
    "# Convert fingerprint list into separate columns (feature matrix)\n",
    "X = np.array(df['Fingerprint'].tolist())\n",
    "\n",
    "# Target variable (Active = 1, Inactive = 0)\n",
    "y = (df['Activity'] == 'Active').astype(int)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=2000),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=200, max_depth=7, learning_rate=0.05, random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(n_estimators=200, max_depth=7, learning_rate=0.05, random_state=42),\n",
    "    \"SVM\": SVC(kernel='rbf', probability=True),\n",
    "    \"MLP (Neural Network)\": MLPClassifier(hidden_layer_sizes=(512, 256), max_iter=300, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} Accuracy: {acc:.4f}\")\n",
    "    \n",
    "    results[name] = acc\n",
    "\n",
    "# Print best model\n",
    "best_model = max(results, key=results.get)\n",
    "print(f\"\\nBest model: {best_model} with accuracy: {results[best_model]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d9c980-d566-4e3d-b200-a9ee4d95d189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea199944-fae8-4726-9c16-edbd6d6ef251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00bc7bd1-d703-4e83-bfc1-594a03e6518d",
   "metadata": {},
   "source": [
    "# classification prediction using pretrained model chemberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "376b61c1-54b0-4253-990b-7563a2a7b1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ngaga\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3780' max='3780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3780/3780 04:24, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.612600</td>\n",
       "      <td>0.589709</td>\n",
       "      <td>0.681909</td>\n",
       "      <td>0.681259</td>\n",
       "      <td>0.668456</td>\n",
       "      <td>0.674797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.548800</td>\n",
       "      <td>0.537832</td>\n",
       "      <td>0.722333</td>\n",
       "      <td>0.724518</td>\n",
       "      <td>0.706040</td>\n",
       "      <td>0.715160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.490300</td>\n",
       "      <td>0.511300</td>\n",
       "      <td>0.751491</td>\n",
       "      <td>0.747989</td>\n",
       "      <td>0.748993</td>\n",
       "      <td>0.748491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.453400</td>\n",
       "      <td>0.503730</td>\n",
       "      <td>0.763419</td>\n",
       "      <td>0.753927</td>\n",
       "      <td>0.773154</td>\n",
       "      <td>0.763419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.484200</td>\n",
       "      <td>0.500911</td>\n",
       "      <td>0.767396</td>\n",
       "      <td>0.748737</td>\n",
       "      <td>0.795973</td>\n",
       "      <td>0.771633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.451300</td>\n",
       "      <td>0.490734</td>\n",
       "      <td>0.781312</td>\n",
       "      <td>0.769831</td>\n",
       "      <td>0.794631</td>\n",
       "      <td>0.782034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.438400</td>\n",
       "      <td>0.490858</td>\n",
       "      <td>0.785951</td>\n",
       "      <td>0.768448</td>\n",
       "      <td>0.810738</td>\n",
       "      <td>0.789027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.465200</td>\n",
       "      <td>0.488821</td>\n",
       "      <td>0.787939</td>\n",
       "      <td>0.767296</td>\n",
       "      <td>0.818792</td>\n",
       "      <td>0.792208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.435400</td>\n",
       "      <td>0.485204</td>\n",
       "      <td>0.790590</td>\n",
       "      <td>0.772554</td>\n",
       "      <td>0.816107</td>\n",
       "      <td>0.793734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.455200</td>\n",
       "      <td>0.486388</td>\n",
       "      <td>0.789927</td>\n",
       "      <td>0.770202</td>\n",
       "      <td>0.818792</td>\n",
       "      <td>0.793754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned ChemBERTa model saved!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\aragen\\AChE_ligands_weighted_activity.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop NaNs\n",
    "df = df.dropna(subset=['SMILES', 'Activity'])\n",
    "\n",
    "# Convert labels to binary (Active = 1, Inactive = 0)\n",
    "df['Activity'] = df['Activity'].map({'Active': 1, 'Inactive': 0})\n",
    "\n",
    "# Train-test split\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['SMILES'].tolist(), df['Activity'].tolist(), test_size=0.2, random_state=42, stratify=df['Activity']\n",
    ")\n",
    "\n",
    "# Load ChemBERTa tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\")\n",
    "\n",
    "# Create Dataset class\n",
    "class SmilesDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(self.texts[idx], truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\")\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = SmilesDataset(train_texts, train_labels, tokenizer)\n",
    "test_dataset = SmilesDataset(test_texts, test_labels, tokenizer)\n",
    "\n",
    "# Load pre-trained ChemBERTa model for classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\", num_labels=2)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./chemberta_results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "# Define evaluation metric\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = torch.argmax(torch.tensor(logits), dim=-1).numpy()\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"./fine_tuned_chemberta\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_chemberta\")\n",
    "print(\"Fine-tuned ChemBERTa model saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0e0d80-d659-448c-bde6-12a840c3e612",
   "metadata": {},
   "source": [
    "# IT CAN BE SEEN THAT XGBOOST HAS BEEN PERFORMING WELL COMPARED TO OTHERS BEFORE FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2250e7-e0d1-4284-9454-638cb9d45cc0",
   "metadata": {},
   "source": [
    "## feature extraction (descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "20b59e85-e6c0-4ac6-9680-9f5ecb2d1589",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:15:10] SMILES Parse Error: syntax error while parsing: nan\n",
      "[14:15:10] SMILES Parse Error: Failed parsing SMILES 'nan' for input: 'nan'\n",
      "[14:15:15] SMILES Parse Error: syntax error while parsing: nan\n",
      "[14:15:15] SMILES Parse Error: Failed parsing SMILES 'nan' for input: 'nan'\n",
      "[14:15:15] SMILES Parse Error: syntax error while parsing: nan\n",
      "[14:15:15] SMILES Parse Error: Failed parsing SMILES 'nan' for input: 'nan'\n",
      "[14:15:15] SMILES Parse Error: syntax error while parsing: nan\n",
      "[14:15:15] SMILES Parse Error: Failed parsing SMILES 'nan' for input: 'nan'\n",
      "[14:15:15] SMILES Parse Error: syntax error while parsing: nan\n",
      "[14:15:15] SMILES Parse Error: Failed parsing SMILES 'nan' for input: 'nan'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Descriptors added and dataset saved!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, rdMolDescriptors\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\aragen\\AChE_ligands_weighted_activity.csv\")  # Replace with actual dataset path\n",
    "\n",
    "# Ensure SMILES column is of string type and handle missing values\n",
    "df[\"SMILES\"] = df[\"SMILES\"].astype(str).str.strip()  # Convert to string & remove spaces\n",
    "df = df.dropna(subset=[\"SMILES\"])  # Drop rows where SMILES is NaN\n",
    "\n",
    "# Function to compute descriptors\n",
    "def compute_descriptors(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return [np.nan] * 6  # Return NaN if invalid SMILES\n",
    "\n",
    "    return [\n",
    "        Descriptors.MolWt(mol),                        # Molecular Weight\n",
    "        Descriptors.MolLogP(mol),                      # LogP\n",
    "        Descriptors.TPSA(mol),                         # Topological Polar Surface Area\n",
    "        rdMolDescriptors.CalcNumHBA(mol),              # Hydrogen Bond Acceptors\n",
    "        rdMolDescriptors.CalcNumHBD(mol),              # Hydrogen Bond Donors\n",
    "        rdMolDescriptors.CalcNumRotatableBonds(mol),   # Rotatable Bonds\n",
    "    ]\n",
    "\n",
    "# Apply descriptor calculation safely\n",
    "df[[\"MolWt\", \"LogP\", \"TPSA\", \"HBA\", \"HBD\", \"RotBonds\"]] = df[\"SMILES\"].apply(\n",
    "    lambda x: pd.Series(compute_descriptors(x) if isinstance(x, str) else [np.nan] * 6)\n",
    ")\n",
    "\n",
    "# Save the updated dataset\n",
    "df.to_csv(\"dataset_with_descriptors.csv\", index=False)\n",
    "\n",
    "print(\" Descriptors added and dataset saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea76fcd-eda5-44ac-b299-e4f33dab6e20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5e54190-d857-436f-a744-04fd2bd08222",
   "metadata": {},
   "source": [
    "# training multiple ML models with feature extracted dataset to find the best model before normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e2f151c-4f02-4a89-92f0-a6a19858abc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training Logistic Regression...\n",
      " Training Random Forest...\n",
      " Training Gradient Boosting...\n",
      " Training Decision Tree...\n",
      " Training SVM...\n",
      " Training K-Nearest Neighbors...\n",
      " Training XGBoost...\n",
      " Training LightGBM...\n",
      "[LightGBM] [Info] Number of positive: 2980, number of negative: 3055\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000209 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 827\n",
      "[LightGBM] [Info] Number of data points in the train set: 6035, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493786 -> initscore=-0.024856\n",
      "[LightGBM] [Info] Start training from score -0.024856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngaga\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [14:28:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\ngaga\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training CatBoost...\n",
      "\n",
      " Model Performance Comparison:\n",
      "                 Model  Accuracy  Precision    Recall  F1-Score\n",
      "1        Random Forest  0.804506   0.804054  0.798658  0.801347\n",
      "6              XGBoost  0.781312   0.784636  0.767785  0.776119\n",
      "5  K-Nearest Neighbors  0.773360   0.758665  0.793289  0.775591\n",
      "7             LightGBM  0.778661   0.785814  0.758389  0.771858\n",
      "8             CatBoost  0.768721   0.774238  0.750336  0.762100\n",
      "3        Decision Tree  0.752816   0.742820  0.763758  0.753144\n",
      "2    Gradient Boosting  0.707091   0.713681  0.679195  0.696011\n",
      "4                  SVM  0.672631   0.666667  0.673826  0.670227\n",
      "0  Logistic Regression  0.582505   0.587253  0.519463  0.551282\n",
      "\n",
      " Best model based on F1-score: Random Forest\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(R\"C:\\Users\\ngaga\\OneDrive\\Desktop\\aragen\\AChE_dataset\\dataset_with_descriptors.csv\")\n",
    "\n",
    "# Drop any missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Convert Activity column to binary (Active = 1, Inactive = 0)\n",
    "df[\"Activity\"] = df[\"Activity\"].map({\"Active\": 1, \"Inactive\": 0})\n",
    "\n",
    "# Define feature columns (excluding non-numeric columns)\n",
    "feature_cols = [\"MolWt\", \"LogP\", \"TPSA\", \"HBA\", \"HBD\", \"RotBonds\"]\n",
    "X = df[feature_cols]  # Features\n",
    "y = df[\"Activity\"]  # Target\n",
    "\n",
    "# Split into training and testing sets (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Standardize features (important for models like SVM & Logistic Regression)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define multiple ML models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\"),\n",
    "    \"LightGBM\": LGBMClassifier(),\n",
    "    \"CatBoost\": CatBoostClassifier(verbose=0)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\" Training {name}...\")\n",
    "    \n",
    "    model.fit(X_train, y_train)  # Train model\n",
    "    y_pred = model.predict(X_test)  # Predict on test set\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\"Model\": name, \"Accuracy\": accuracy, \"Precision\": precision, \"Recall\": recall, \"F1-Score\": f1})\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"F1-Score\", ascending=False)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n Model Performance Comparison:\")\n",
    "print(results_df)\n",
    "\n",
    "# Save results to CSV\n",
    "results_df.to_csv(\"model_comparison_results.csv\", index=False)\n",
    "\n",
    "print(\"\\n Best model based on F1-score:\", results_df.iloc[0][\"Model\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a76c07-994e-401a-921d-2562f0cd6c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa688072-d8f6-4677-8e88-82b571a7fca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3d7e37b-6200-407b-8e8b-6593abcc209b",
   "metadata": {},
   "source": [
    "# finetuning chemberta model to get higher accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8efbf3c-8598-4695-aa55-fe101c36a7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset prepared successfully!\n",
      " Model loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 189/189 [19:11<00:00,  6.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Loss: 0.5617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 189/189 [20:51<00:00,  6.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 - Loss: 0.4223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 189/189 [20:53<00:00,  6.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 - Loss: 0.3531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 189/189 [32:18<00:00, 10.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 - Loss: 0.3023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 189/189 [16:53<00:00,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 - Loss: 0.2677\n",
      " Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.8376 | Precision: 0.8238 | Recall: 0.8537 | F1 Score: 0.8385\n"
     ]
    }
   ],
   "source": [
    "# Required Libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device (CPU only)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\aragen\\AChE_dataset\\dataset_with_descriptors.csv\")\n",
    "\n",
    "# Drop missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Define input features (including SMILES and molecular descriptors)\n",
    "feature_columns = [\"MolWt\", \"LogP\", \"TPSA\", \"HBA\", \"HBD\", \"RotBonds\"]  # Molecular descriptors\n",
    "X_features = df[feature_columns].values  # Extract numerical features\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_features = scaler.fit_transform(X_features)\n",
    "\n",
    "# Encode Activity label (0 = Inactive, 1 = Active)\n",
    "df[\"Activity\"] = df[\"Activity\"].map({\"Inactive\": 0, \"Active\": 1})\n",
    "y = df[\"Activity\"].values  # Target variable\n",
    "\n",
    "# Train-test split (80-20)\n",
    "X_train, X_test, y_train, y_test, train_features, test_features = train_test_split(\n",
    "    df[\"SMILES\"], y, X_features, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Load ChemBERTa tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "\n",
    "# Define dataset class\n",
    "class ChemDataset(Dataset):\n",
    "    def __init__(self, smiles_list, features, labels, tokenizer, max_length=128):\n",
    "        self.smiles_list = smiles_list\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        smiles = self.smiles_list.iloc[idx]\n",
    "        label = self.labels[idx]\n",
    "        feature = self.features[idx]\n",
    "\n",
    "        # Tokenize SMILES\n",
    "        encoding = self.tokenizer(\n",
    "            smiles,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"features\": feature,\n",
    "            \"labels\": label,\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ChemDataset(X_train, train_features, y_train, tokenizer)\n",
    "test_dataset = ChemDataset(X_test, test_features, y_test, tokenizer)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(\" Dataset prepared successfully!\")\n",
    "\n",
    "# Define ChemBERTa-based model with numerical feature fusion\n",
    "class ChemBERTaWithFeatures(nn.Module):\n",
    "    def __init__(self, bert_model_name=\"seyonec/ChemBERTa-zinc-base-v1\", feature_dim=6, num_classes=2):\n",
    "        super(ChemBERTaWithFeatures, self).__init__()\n",
    "        self.chemberta = RobertaModel.from_pretrained(bert_model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.feature_fc = nn.Linear(feature_dim, 64)  # Project numerical features\n",
    "        self.classifier = nn.Linear(self.chemberta.config.hidden_size + 64, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, features):\n",
    "        bert_output = self.chemberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        chemberta_embedding = bert_output.pooler_output  # Get pooled embedding\n",
    "\n",
    "        # Process numerical features\n",
    "        feature_output = torch.relu(self.feature_fc(features))\n",
    "\n",
    "        # Concatenate ChemBERTa embedding & extracted features\n",
    "        combined = torch.cat((chemberta_embedding, feature_output), dim=1)\n",
    "\n",
    "        # Final classification layer\n",
    "        output = self.classifier(self.dropout(combined))\n",
    "\n",
    "        return output\n",
    "\n",
    "# Initialize model\n",
    "model = ChemBERTaWithFeatures()\n",
    "\n",
    "print(\" Model loaded successfully!\")\n",
    "\n",
    "# Training setup\n",
    "epochs = 5\n",
    "learning_rate = 2e-5\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        features = batch[\"features\"]\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask, features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "print(\" Training complete!\")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    preds, true_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            attention_mask = batch[\"attention_mask\"]\n",
    "            features = batch[\"features\"]\n",
    "            labels = batch[\"labels\"]\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, features)\n",
    "            predictions = torch.argmax(outputs, dim=1).numpy()\n",
    "\n",
    "            preds.extend(predictions)\n",
    "            true_labels.extend(labels.numpy())\n",
    "\n",
    "    acc = accuracy_score(true_labels, preds)\n",
    "    prec = precision_score(true_labels, preds)\n",
    "    rec = recall_score(true_labels, preds)\n",
    "    f1 = f1_score(true_labels, preds)\n",
    "\n",
    "    print(f\" Accuracy: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "evaluate(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6c8a27-916f-4d44-af98-8abde8d1aec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c80bd1-a3e2-4085-b155-57c3ae57287a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7be70ec-d1ee-4864-8cbf-aae0561942b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d71e0b32-0175-4585-b082-e02d2f6e11f8",
   "metadata": {},
   "source": [
    "# XGBOOST model using just smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d38338a9-ece5-4dba-a607-0bb877dd5a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training XGBoost...\n",
      "\n",
      "XGBoost Accuracy: 0.8588\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.88      0.86       764\n",
      "           1       0.87      0.84      0.85       745\n",
      "\n",
      "    accuracy                           0.86      1509\n",
      "   macro avg       0.86      0.86      0.86      1509\n",
      "weighted avg       0.86      0.86      0.86      1509\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\aragen\\AChE_dataset\\AChE_ligands_weighted_activity.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert SMILES to Morgan Fingerprints (2048-bit vector)\n",
    "def smiles_to_fingerprint(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        return list(AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048))\n",
    "    else:\n",
    "        return [0] * 2048  # Return zero vector if conversion fails\n",
    "\n",
    "# Drop NaN SMILES values\n",
    "df = df.dropna(subset=['SMILES'])\n",
    "\n",
    "# Generate fingerprints\n",
    "df['Fingerprint'] = df['SMILES'].apply(smiles_to_fingerprint)\n",
    "\n",
    "# Convert fingerprint list into separate columns (feature matrix)\n",
    "X = np.array(df['Fingerprint'].tolist())\n",
    "\n",
    "# Target variable (Active = 1, Inactive = 0)\n",
    "y = (df['Activity'] == 'Active').astype(int)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize XGBoost model\n",
    "xgb_model = XGBClassifier(n_estimators=200, max_depth=7, learning_rate=0.05, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nTraining XGBoost...\")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nXGBoost Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfae66e-7302-4542-acbe-8d931e232125",
   "metadata": {},
   "source": [
    "# XGBOOST using smiles and molecular descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc264b9f-ec18-4bae-8e36-a1b28682ef5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training XGBoost on SMILES + Molecular Descriptors...\n",
      "\n",
      "XGBoost Accuracy: 0.8588\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86       764\n",
      "           1       0.88      0.83      0.85       745\n",
      "\n",
      "    accuracy                           0.86      1509\n",
      "   macro avg       0.86      0.86      0.86      1509\n",
      "weighted avg       0.86      0.86      0.86      1509\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\aragen\\AChE_dataset\\dataset_with_descriptors.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop missing values\n",
    "df.dropna(subset=['SMILES'], inplace=True)  \n",
    "\n",
    "# Convert SMILES to Morgan Fingerprints (2048-bit vector)\n",
    "def smiles_to_fingerprint(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        return list(AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048))\n",
    "    else:\n",
    "        return [0] * 2048  # Return zero vector if conversion fails\n",
    "\n",
    "# Generate fingerprints\n",
    "df['Fingerprint'] = df['SMILES'].apply(smiles_to_fingerprint)\n",
    "\n",
    "# Extract molecular descriptors as features\n",
    "descriptor_columns = [\"MolWt\", \"LogP\", \"TPSA\", \"HBA\", \"HBD\", \"RotBonds\"]\n",
    "X_descriptors = df[descriptor_columns].values  # Convert to NumPy array\n",
    "\n",
    "# Convert fingerprint lists into NumPy array\n",
    "X_fingerprints = np.array(df['Fingerprint'].tolist())\n",
    "\n",
    "# Concatenate fingerprints and molecular descriptors\n",
    "X = np.hstack((X_fingerprints, X_descriptors))  # Combine both feature sets\n",
    "\n",
    "# Target variable (Active = 1, Inactive = 0)\n",
    "y = (df[\"Activity\"] == \"Active\").astype(int)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize XGBoost model\n",
    "xgb_model = XGBClassifier(n_estimators=200, max_depth=7, learning_rate=0.05, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nTraining XGBoost on SMILES + Molecular Descriptors...\")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nXGBoost Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f4c78a-bdbf-4b42-ba62-c737e2834228",
   "metadata": {},
   "source": [
    "# XGBOOST after normalizing the molecular descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b0325e95-8bbd-4f82-b13a-a0f977ab3b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training XGBoost on SMILES + Normalized Molecular Descriptors...\n",
      "\n",
      "XGBoost Accuracy: 0.8588\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86       764\n",
      "           1       0.88      0.83      0.85       745\n",
      "\n",
      "    accuracy                           0.86      1509\n",
      "   macro avg       0.86      0.86      0.86      1509\n",
      "weighted avg       0.86      0.86      0.86      1509\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\aragen\\AChE_dataset\\dataset_with_descriptors.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop missing values\n",
    "df.dropna(subset=['SMILES'], inplace=True)  \n",
    "\n",
    "# Convert SMILES to Morgan Fingerprints (2048-bit vector)\n",
    "def smiles_to_fingerprint(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        return list(AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048))\n",
    "    else:\n",
    "        return [0] * 2048  # Return zero vector if conversion fails\n",
    "\n",
    "# Generate fingerprints\n",
    "df['Fingerprint'] = df['SMILES'].apply(smiles_to_fingerprint)\n",
    "\n",
    "# Extract molecular descriptors as features\n",
    "descriptor_columns = [\"MolWt\", \"LogP\", \"TPSA\", \"HBA\", \"HBD\", \"RotBonds\"]\n",
    "X_descriptors = df[descriptor_columns].values  # Convert to NumPy array\n",
    "\n",
    "# Normalize molecular descriptors\n",
    "scaler = StandardScaler()\n",
    "X_descriptors_scaled = scaler.fit_transform(X_descriptors)\n",
    "\n",
    "# Convert fingerprint lists into NumPy array\n",
    "X_fingerprints = np.array(df['Fingerprint'].tolist())\n",
    "\n",
    "# Concatenate fingerprints and normalized molecular descriptors\n",
    "X = np.hstack((X_fingerprints, X_descriptors_scaled))  # Combine both feature sets\n",
    "\n",
    "# Target variable (Active = 1, Inactive = 0)\n",
    "y = (df[\"Activity\"] == \"Active\").astype(int)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize XGBoost model\n",
    "xgb_model = XGBClassifier(n_estimators=200, max_depth=7, learning_rate=0.05, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nTraining XGBoost on SMILES + Normalized Molecular Descriptors...\")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nXGBoost Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb28e79-555a-4417-b57d-f097e07d9b5a",
   "metadata": {},
   "source": [
    "### we can see that even after adding molecular descriptors the model is performing the same because the model can grasp all of those features from the smiles itself. Hence proceeding with the smiles dataset instead of depending on molecular descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245e471c-8fc7-4a75-91d8-8616ac759f4c",
   "metadata": {},
   "source": [
    "# predicting random smile if its active or inactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4988dd23-d979-4c21-8bf9-901f8e43340b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for NC(=O)c1ccc(CNc2ccc(OCC3CCCCC3)cc2)cc1: Inactive\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# Load the trained model\n",
    "loaded_model = joblib.load(\"xgboost_AChE_model.pkl\")\n",
    "\n",
    "# Function to predict activity from a SMILES string\n",
    "def predict_activity(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        fingerprint = list(AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048))\n",
    "        fingerprint = np.array(fingerprint).reshape(1, -1)  # Reshape for prediction\n",
    "        prediction = loaded_model.predict(fingerprint)\n",
    "        return \"Active\" if prediction[0] == 1 else \"Inactive\"\n",
    "    else:\n",
    "        return \"Invalid SMILES\"\n",
    "\n",
    "# Example usage\n",
    "smiles_input = \"NC(=O)c1ccc(CNc2ccc(OCC3CCCCC3)cc2)cc1\"\n",
    "print(f\"Prediction for {smiles_input}: {predict_activity(smiles_input)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a03a762-452e-45f8-a3bb-c587fc150da5",
   "metadata": {},
   "source": [
    "# prediction for smiles with a txt file with confidence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0ada21de-dc40-4c8d-b740-82b481fcb4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions:\n",
      "\n",
      "SMILES                                             Activity   Confidence\n",
      "---------------------------------------------------------------------------\n",
      "COc1ccc2c(c1)CCC(C1CCN(c3ccccc3)CC1)C2             Inactive   0.52\n",
      "COc1cc2c(cc1OC)C(=O)C(CC1CCN(Cc3ccccc3)CC1)C2      Active     0.91\n",
      "O=C(NCC1CCN(Cc2ccccc2)CC1)c1cc(=O)c2cc(O)ccc2o1    Inactive   0.69\n",
      "O=C(CCc1ccccc1)NCC1CCN(Cc2ccccc2)CC1               Inactive   0.65\n",
      "COc1cc2cc(-c3ccc(CN(C)Cc4ccccc4)cc3)c(=O)oc2cc1OC  Active     0.81\n",
      "COc1cc2c(cc1OC)C(=O)/C(=C/c1ccc(N3CCCC3)cc1)C2     Active     0.70\n",
      "CCNCC(=O)Nc1ccc2nc3n(c(=O)c2c1)CCC3                Active     0.82\n",
      "Nc1c2c(nc3ccccc13)CCCC2                            Active     0.92\n",
      "COc1cc2c(cc1OC)C(=O)/C(=C/c1cc[n+](Cc3cccc(Cl)c3)cc1C(C)=O)C2.[Br-] Active     0.85\n",
      "CNc1c2c(nc3ccccc13)CCCC2                           Active     0.77\n",
      "Nc1c2c(nc3cc(Cl)ccc13)CCCC2                        Active     0.93\n",
      "Cc1ccc2nc(C(=O)NCC3CCN(Cc4ccccc4)CC3)ccc2c1        Inactive   0.69\n",
      "COc1cc2c(cc1OC)C(=O)C(CC1CCN(Cc3ccccc3)CC1)C2      Active     0.91\n",
      "CS(=O)(=O)C1=CC=C2ON=C(NCC3CCN(Cc4ccccc4)CC3)N=C2C=C1 Inactive   0.61\n",
      "Nc1c2c(nc3ccccc13)CCCC2                            Active     0.92\n",
      "Nc1c2c(nc3ccccc13)CCCC2                            Active     0.92\n",
      "COc1cc2c(cc1OC)C(=O)C(CC1CCN(Cc3ccccc3)CC1)C2      Active     0.91\n",
      "CCN(CCCCc1c2c(nc3ccccc13)CCCC2)NCCCCc1c2c(nc3ccccc13)CCCC2 Active     0.87\n",
      "COCCOC1=C(C)Nc2nc3c(c(N)c2[C@H]1c1ccccc1)CCCC3     Active     0.64\n",
      "COc1cc2c(cc1OC)C(=O)C(CC1CCN(Cc3ccccc3)CC1)C2      Active     0.91\n",
      "COc1cc2cc(NC(=O)C3CCN(Cc4ccccc4)CC3)sc2cc1OC       Active     0.62\n",
      "Nc1c2c(nc3ccccc13)CCCC2                            Active     0.92\n",
      "CC1=CC2Cc3nc4cc(Cl)ccc4c(N)c3C(C1)C2.Cl            Active     0.91\n",
      "CNC(=O)Oc1ccc2c(c1)[C@]1(C)CCN(C)C1N2Cc1ccccc1     Active     0.76\n",
      "CNC(=O)Oc1ccc2c(c1)[C@]1(C)CCN(C)[C@@H]1N2C        Active     0.82\n",
      "CCCCCCNC(=O)Oc1ccc(Cc2nc3cc(C(=O)N(CC)CC)ccc3n2CCC(C)C)cc1 Inactive   0.63\n",
      "CCN(C)C1=CC=C(Cc2cccc(OC(=O)NCCCCCc3ccccc3)c2)/C=C2\\CCc3ccccc3NC2C1 Inactive   0.58\n",
      "COc1cc2cc(C(=O)C3CCC[N+](C)(Cc4ccccc4)CC3)sc2cc1OC Active     0.82\n",
      "COC(=O)Nc1ccc2nc3n(c(=O)c2c1)CCC3                  Inactive   0.53\n",
      "CCNC(=O)c1ccc2nc3n(c(=O)c2c1)CCC3                  Active     0.50\n",
      "COc1cc2c(cc1OC)C(=O)C(Cc1cccc(CN3CCCCC3)c1)C2      Active     0.89\n",
      "NS(=O)(=O)c1ccc2c(c1)CN(C(=O)c1ccccc1)C2           Active     0.75\n",
      "CCCNc1c2c(nc3ccc(Cl)cc13)CCCC2                     Inactive   0.62\n",
      "Nc1c2c(nc3cc(Cl)ccc13)CCCC2                        Active     0.93\n",
      "CCOC(=O)c1ccc2c(CCC3CCN(Cc4ccccc4)CC3)noc2c1       Active     0.94\n",
      "O=C(Cc1cc(=O)oc2cc(O)ccc12)NCCCNc1c2c(nc3ccccc13)CCCC2 Active     0.88\n",
      "NC(=O)C1=Cc2cc(OCCCC3CCN(Cc4ccccc4)CC3)ccc21       Inactive   0.54\n",
      "Nc1c2c(nc3ccccc13)CCCC2                            Active     0.92\n",
      "CC1=CC=C2C(C(=O)CCC3CCN(Cc4ccccc4)CC3)=CC=C12      Active     0.92\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# Load the trained model\n",
    "loaded_model = joblib.load(\"xgboost_AChE_model.pkl\")\n",
    "\n",
    "# Function to predict activity and confidence score from a SMILES string\n",
    "def predict_activity(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        fingerprint = list(AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048))\n",
    "        fingerprint = np.array(fingerprint).reshape(1, -1)  # Reshape for prediction\n",
    "        \n",
    "        # Get prediction and confidence score\n",
    "        prediction = loaded_model.predict(fingerprint)[0]\n",
    "        confidence = loaded_model.predict_proba(fingerprint)[0][prediction]  # Probability of the predicted class\n",
    "        \n",
    "        activity = \"Active\" if prediction == 1 else \"Inactive\"\n",
    "        return activity, confidence\n",
    "    else:\n",
    "        return \"Invalid SMILES\", None\n",
    "\n",
    "# Function to process a file with multiple SMILES and display results\n",
    "def process_smiles_file(input_file):\n",
    "    with open(input_file, 'r') as f:\n",
    "        smiles_list = [line.strip() for line in f.readlines()]  # Read all SMILES from file\n",
    "\n",
    "    print(\"\\nPredictions:\\n\")\n",
    "    print(f\"{'SMILES':<50} {'Activity':<10} {'Confidence':<10}\")\n",
    "    print(\"-\" * 75)\n",
    "\n",
    "    for smiles in smiles_list:\n",
    "        activity, confidence = predict_activity(smiles)\n",
    "        if confidence is not None:\n",
    "            print(f\"{smiles:<50} {activity:<10} {confidence:.2f}\")\n",
    "        else:\n",
    "            print(f\"{smiles:<50} {'Invalid':<10} {'N/A':<10}\")\n",
    "\n",
    "# Example usage\n",
    "input_file = r\"C:\\Users\\ngaga\\Downloads\\valid_molecules.txt\"  # Input file with multiple SMILES (one per line)\n",
    "process_smiles_file(input_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc472375-2ef7-4917-8f92-e9ab31561025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64e2ecd-74a1-40c6-92f1-1aff54400f87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b787bd34-841e-4531-91f6-50ef07e889a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
