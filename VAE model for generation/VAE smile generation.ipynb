{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class SimpleCharVAE(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=256, latent_size=64):\n",
    "        super(SimpleCharVAE, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.encoder_rnn = nn.GRU(hidden_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.mu = nn.Linear(hidden_size*2, latent_size)  # *2 for bidirectional\n",
    "        self.logvar = nn.Linear(hidden_size*2, latent_size)\n",
    "\n",
    "        # Decoder\n",
    "        self.latent_to_hidden = nn.Linear(latent_size, hidden_size)\n",
    "        self.decoder_rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.output = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, hidden = self.encoder_rnn(x)\n",
    "        # Combine forward and backward hidden states\n",
    "        hidden = torch.cat([hidden[0], hidden[1]], dim=1)\n",
    "\n",
    "        mu = self.mu(hidden)\n",
    "        logvar = self.logvar(hidden)\n",
    "        return mu, logvar\n",
    "\n",
    "    def decode(self, z, target=None, max_length=100, teacher_forcing=0.5):\n",
    "        batch_size = z.size(0)\n",
    "        device = z.device\n",
    "\n",
    "        # Initialize hidden state from latent vector\n",
    "        hidden = self.latent_to_hidden(z).unsqueeze(0)\n",
    "\n",
    "        # Start with zeros (will be converted to start token)\n",
    "        decoder_input = torch.zeros(batch_size, 1, device=device, dtype=torch.long)\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        # Generate sequence\n",
    "        for t in range(max_length):\n",
    "            # Embed input token\n",
    "            emb = self.embedding(decoder_input)\n",
    "\n",
    "            # Run RNN for one step\n",
    "            output, hidden = self.decoder_rnn(emb, hidden)\n",
    "\n",
    "            # Get output probabilities\n",
    "            output = self.output(output)\n",
    "            outputs.append(output)\n",
    "\n",
    "            # Teacher forcing or use own prediction\n",
    "            if target is not None and t < target.size(1) and random.random() < teacher_forcing:\n",
    "                decoder_input = target[:, t:t+1]\n",
    "            else:\n",
    "                decoder_input = output.argmax(2)\n",
    "\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x, target=None):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        output = self.decode(z, target)\n",
    "        return output, mu, logvar\n",
    "\n",
    "    def sample(self, n_samples=1, device='cuda'):\n",
    "        with torch.no_grad():\n",
    "            # Sample from latent space\n",
    "            z = torch.randn(n_samples, self.mu.out_features, device=device)\n",
    "            # Decode\n",
    "            output = self.decode(z, teacher_forcing=0)\n",
    "            # Get most likely tokens\n",
    "            return output.argmax(dim=2)\n",
    "\n",
    "def create_char_dict(smiles_list):\n",
    "    \"\"\"Create character to index mapping from SMILES strings\"\"\"\n",
    "    # Add start/end tokens\n",
    "    chars = set()\n",
    "    for smiles in smiles_list:\n",
    "        chars.update(smiles)\n",
    "\n",
    "    # Special tokens\n",
    "    all_chars = ['<pad>', '<start>', '<end>'] + sorted(list(chars))\n",
    "    char_to_idx = {c: i for i, c in enumerate(all_chars)}\n",
    "    idx_to_char = {i: c for c, i in char_to_idx.items()}\n",
    "\n",
    "    return char_to_idx, idx_to_char, all_chars\n",
    "\n",
    "def tokenize_smiles(smiles, char_to_idx, max_length=100):\n",
    "    \"\"\"Convert SMILES to token indices\"\"\"\n",
    "    tokens = [char_to_idx['<start>']]\n",
    "\n",
    "    # Add character tokens\n",
    "    for c in smiles[:max_length-2]:  # -2 for start/end tokens\n",
    "        tokens.append(char_to_idx.get(c, char_to_idx['<pad>']))\n",
    "\n",
    "    # Add end token\n",
    "    tokens.append(char_to_idx['<end>'])\n",
    "\n",
    "    # Pad to fixed length\n",
    "    while len(tokens) < max_length:\n",
    "        tokens.append(char_to_idx['<pad>'])\n",
    "\n",
    "    return tokens[:max_length]  # Ensure max length\n",
    "\n",
    "def smiles_to_tensor(smiles_list, char_to_idx, max_length=100):\n",
    "    \"\"\"Convert list of SMILES to tensor of token indices\"\"\"\n",
    "    tensors = []\n",
    "    for smiles in smiles_list:\n",
    "        tensors.append(tokenize_smiles(smiles, char_to_idx, max_length))\n",
    "    return torch.tensor(tensors, dtype=torch.long)\n",
    "\n",
    "def tensor_to_smiles(tensor, idx_to_char):\n",
    "    \"\"\"Convert tensor of token indices back to SMILES\"\"\"\n",
    "    smiles = []\n",
    "    for t in tensor:\n",
    "        chars = []\n",
    "        for i in t:\n",
    "            c = idx_to_char[i.item()]\n",
    "            if c == '<end>':\n",
    "                break\n",
    "            if c not in ['<pad>', '<start>']:\n",
    "                chars.append(c)\n",
    "        smiles.append(''.join(chars))\n",
    "    return smiles\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar, kl_weight=0.1):\n",
    "    \"\"\"Combined loss function for VAE with KL annealing\"\"\"\n",
    "    # Reconstruction loss (cross entropy)\n",
    "    recon_x_flat = recon_x.reshape(-1, recon_x.size(2))\n",
    "    x_flat = x.reshape(-1)\n",
    "\n",
    "    # Mask padding in loss calculation\n",
    "    mask = (x_flat != 0)  # Assuming 0 is pad token\n",
    "    recon_loss = F.cross_entropy(recon_x_flat[mask], x_flat[mask], reduction='sum')\n",
    "\n",
    "    # KL divergence\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    # Total loss\n",
    "    return recon_loss + kl_weight * kl_loss\n",
    "\n",
    "def train_model(model, train_data, optimizer, device, epochs=10):\n",
    "    \"\"\"Train the VAE model\"\"\"\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Process in batches\n",
    "        batch_size = 64\n",
    "        num_batches = len(train_data) // batch_size\n",
    "\n",
    "        for i in tqdm(range(num_batches), desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            # Get batch\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            batch = train_data[start_idx:end_idx].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            recon_batch, mu, logvar = model(batch, batch)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = vae_loss(recon_batch, batch, mu, logvar)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / (num_batches * batch_size)\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return train_losses\n",
    "\n",
    "def generate_smiles(model, idx_to_char, n_samples=100, device='cuda'):\n",
    "    \"\"\"Generate SMILES strings from the model\"\"\"\n",
    "    model.eval()\n",
    "    generated = []\n",
    "\n",
    "    # Generate in batches for efficiency\n",
    "    batch_size = 50\n",
    "    n_batches = (n_samples + batch_size - 1) // batch_size\n",
    "\n",
    "    for _ in range(n_batches):\n",
    "        with torch.no_grad():\n",
    "            # Sample from model\n",
    "            samples = model.sample(min(batch_size, n_samples - len(generated)), device)\n",
    "\n",
    "            # Convert to SMILES\n",
    "            batch_smiles = tensor_to_smiles(samples, idx_to_char)\n",
    "\n",
    "            # Save all generated strings even if they're not valid\n",
    "            for smiles in batch_smiles:\n",
    "                if smiles not in generated and len(smiles) <= 100:\n",
    "                    generated.append(smiles)\n",
    "                    if len(generated) >= n_samples:\n",
    "                        break\n",
    "\n",
    "    return generated\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters\n",
    "    max_length = 100\n",
    "    hidden_size = 256\n",
    "    latent_size = 64\n",
    "    epochs = 100\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load data\n",
    "    data_file = \"/content/dataset_with_descriptors.csv\"\n",
    "    df = pd.read_csv(data_file)\n",
    "\n",
    "    # Filter for active compounds\n",
    "    active_df = df[df['Activity'] == 'Active']\n",
    "    print(f\"Found {len(active_df)} active compounds\")\n",
    "\n",
    "    # Get SMILES\n",
    "    active_smiles = active_df['SMILES'].dropna().tolist()\n",
    "\n",
    "    # Create character mappings\n",
    "    char_to_idx, idx_to_char, all_chars = create_char_dict(active_smiles)\n",
    "    print(f\"Vocabulary size: {len(all_chars)}\")\n",
    "\n",
    "    # Convert to tensors\n",
    "    all_data = smiles_to_tensor(active_smiles, char_to_idx, max_length)\n",
    "    print(f\"Training on {len(all_data)} examples\")\n",
    "\n",
    "    # Create model\n",
    "    model = SimpleCharVAE(len(all_chars), hidden_size, latent_size).to(device)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train model - use all data, no validation\n",
    "    losses = train_model(model, all_data, optimizer, device, epochs=epochs)\n",
    "\n",
    "    # Plot losses\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.savefig('vae_training_loss.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), 'simple_smiles_vae.pt')\n",
    "\n",
    "    # Generate new SMILES\n",
    "    print(\"Generating new molecules...\")\n",
    "    generated = generate_smiles(model, idx_to_char, n_samples=100, device=device)\n",
    "\n",
    "    # Save all generated SMILES\n",
    "    with open('generated_molecules.txt', 'w') as f:\n",
    "        for smiles in generated:\n",
    "            f.write(f\"{smiles}\\n\")\n",
    "\n",
    "    # Also save molecules validated by RDKit separately\n",
    "    valid_mols = []\n",
    "    for smiles in generated:\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is not None:\n",
    "                valid_mols.append(Chem.MolToSmiles(mol))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    print(f\"Generated {len(generated)} SMILES strings\")\n",
    "    print(f\"Of which {len(valid_mols)} are chemically valid\")\n",
    "\n",
    "    # Save valid molecules separately\n",
    "    with open('valid_molecules.txt', 'w') as f:\n",
    "        for smiles in valid_mols:\n",
    "            f.write(f\"{smiles}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading model to generate new smiles and loading dataset for identifying the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from rdkit import Chem\n",
    "import pandas as pd\n",
    "\n",
    "# Define the model class (same as in your training code)\n",
    "class SimpleCharVAE(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=256, latent_size=64):\n",
    "        super(SimpleCharVAE, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, hidden_size)\n",
    "        self.encoder_rnn = torch.nn.GRU(hidden_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.mu = torch.nn.Linear(hidden_size*2, latent_size)\n",
    "        self.logvar = torch.nn.Linear(hidden_size*2, latent_size)\n",
    "\n",
    "        # Decoder\n",
    "        self.latent_to_hidden = torch.nn.Linear(latent_size, hidden_size)\n",
    "        self.decoder_rnn = torch.nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.output = torch.nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def decode(self, z, target=None, max_length=100, teacher_forcing=0.5):\n",
    "        batch_size = z.size(0)\n",
    "        device = z.device\n",
    "\n",
    "        hidden = self.latent_to_hidden(z).unsqueeze(0)\n",
    "        decoder_input = torch.zeros(batch_size, 1, device=device, dtype=torch.long)\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(max_length):\n",
    "            emb = self.embedding(decoder_input)\n",
    "            output, hidden = self.decoder_rnn(emb, hidden)\n",
    "            output = self.output(output)\n",
    "            outputs.append(output)\n",
    "\n",
    "            if target is not None and t < target.size(1) and random.random() < teacher_forcing:\n",
    "                decoder_input = target[:, t:t+1]\n",
    "            else:\n",
    "                decoder_input = output.argmax(2)\n",
    "\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "    def sample(self, n_samples=1, device='cuda'):\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(n_samples, self.mu.out_features, device=device)\n",
    "            output = self.decode(z, teacher_forcing=0)\n",
    "            return output.argmax(dim=2)\n",
    "\n",
    "# Function to convert tensor to SMILES\n",
    "def tensor_to_smiles(tensor, idx_to_char):\n",
    "    smiles = []\n",
    "    for t in tensor:\n",
    "        chars = []\n",
    "        for i in t:\n",
    "            c = idx_to_char[i.item()]\n",
    "            if c == '<end>':\n",
    "                break\n",
    "            if c not in ['<pad>', '<start>']:\n",
    "                chars.append(c)\n",
    "        smiles.append(''.join(chars))\n",
    "    return smiles\n",
    "\n",
    "# Create character mapping from the original data\n",
    "def create_char_dict(smiles_list):\n",
    "    # Add start/end tokens\n",
    "    chars = set()\n",
    "    for smiles in smiles_list:\n",
    "        chars.update(smiles)\n",
    "\n",
    "    # Special tokens\n",
    "    all_chars = ['<pad>', '<start>', '<end>'] + sorted(list(chars))\n",
    "    char_to_idx = {c: i for i, c in enumerate(all_chars)}\n",
    "    idx_to_char = {i: c for c, i in char_to_idx.items()}\n",
    "\n",
    "    return char_to_idx, idx_to_char, all_chars\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Set paths\n",
    "    model_path = '/content/simple_smiles_vae.pt'  # Path to your saved model\n",
    "    data_file = \"/content/dataset_with_descriptors.csv\"  # Path to original dataset\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load original data to recreate character mapping\n",
    "    print(\"Loading original dataset to recreate character mapping...\")\n",
    "    df = pd.read_csv(data_file)\n",
    "\n",
    "    # Filter for active compounds\n",
    "    active_df = df[df['Activity'] == 'Active']\n",
    "    print(f\"Found {len(active_df)} active compounds\")\n",
    "\n",
    "    # Get SMILES\n",
    "    active_smiles = active_df['SMILES'].dropna().tolist()\n",
    "\n",
    "    # Create character mappings\n",
    "    char_to_idx, idx_to_char, all_chars = create_char_dict(active_smiles)\n",
    "    print(f\"Vocabulary size: {len(all_chars)}\")\n",
    "\n",
    "    # Create model with correct vocabulary size\n",
    "    model = SimpleCharVAE(len(all_chars), hidden_size=256, latent_size=64).to(device)\n",
    "\n",
    "    # Load trained model\n",
    "    print(f\"Loading model from {model_path}...\")\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # Generate SMILES\n",
    "    print(\"Generating molecules...\")\n",
    "    n_samples = 100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        samples = model.sample(n_samples, device)\n",
    "        generated = tensor_to_smiles(samples, idx_to_char)\n",
    "\n",
    "    # Filter valid molecules\n",
    "    valid_mols = []\n",
    "    for smiles in generated:\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is not None:\n",
    "                valid_mols.append(Chem.MolToSmiles(mol))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Save valid molecules\n",
    "    with open('valid_molecules.txt', 'w') as f:\n",
    "        for smiles in valid_mols:\n",
    "            f.write(f\"{smiles}\\n\")\n",
    "\n",
    "    print(f\"Generated {len(generated)} SMILES strings\")\n",
    "    print(f\"Of which {len(valid_mols)} are chemically valid\")\n",
    "\n",
    "    # Save all generated molecules\n",
    "    with open('all_generated_molecules.txt', 'w') as f:\n",
    "        for smiles in generated:\n",
    "            f.write(f\"{smiles}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
